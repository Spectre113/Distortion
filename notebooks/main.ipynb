{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e6a7fe0-1073-4415-ab46-21abc9e5f9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting soundfile\n",
      "  Downloading soundfile-0.13.1-py2.py3-none-manylinux_2_28_x86_64.whl.metadata (16 kB)\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.10/site-packages (from soundfile) (1.16.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from soundfile) (1.26.2)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.0->soundfile) (2.21)\n",
      "Downloading soundfile-0.13.1-py2.py3-none-manylinux_2_28_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25hInstalling collected packages: soundfile\n",
      "Successfully installed soundfile-0.13.1\n"
     ]
    }
   ],
   "source": [
    "!pip install soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d951975-5172-4717-aada-3ed9dec5fcb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting librosa\n",
      "  Downloading librosa-0.11.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting audioread>=2.1.9 (from librosa)\n",
      "  Downloading audioread-3.0.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: numba>=0.51.0 in /opt/conda/lib/python3.10/site-packages (from librosa) (0.58.1)\n",
      "Requirement already satisfied: numpy>=1.22.3 in /opt/conda/lib/python3.10/site-packages (from librosa) (1.26.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from librosa) (1.11.4)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from librosa) (1.3.2)\n",
      "Requirement already satisfied: joblib>=1.0 in /opt/conda/lib/python3.10/site-packages (from librosa) (1.3.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /opt/conda/lib/python3.10/site-packages (from librosa) (0.13.1)\n",
      "Collecting pooch>=1.1 (from librosa)\n",
      "  Downloading pooch-1.8.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting soxr>=0.3.2 (from librosa)\n",
      "  Downloading soxr-1.0.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in /opt/conda/lib/python3.10/site-packages (from librosa) (4.8.0)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in /opt/conda/lib/python3.10/site-packages (from librosa) (0.3)\n",
      "Requirement already satisfied: msgpack>=1.0 in /opt/conda/lib/python3.10/site-packages (from librosa) (1.0.7)\n",
      "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba>=0.51.0->librosa) (0.41.1)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /opt/conda/lib/python3.10/site-packages (from pooch>=1.1->librosa) (4.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from pooch>=1.1->librosa) (23.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from pooch>=1.1->librosa) (2.31.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=1.1.0->librosa) (3.2.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.10/site-packages (from soundfile>=0.12.1->librosa) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2023.11.17)\n",
      "Downloading librosa-0.11.0-py3-none-any.whl (260 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m260.7/260.7 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading audioread-3.0.1-py3-none-any.whl (23 kB)\n",
      "Downloading pooch-1.8.2-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.6/64.6 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading soxr-1.0.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (242 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.3/242.3 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: soxr, audioread, pooch, librosa\n",
      "Successfully installed audioread-3.0.1 librosa-0.11.0 pooch-1.8.2 soxr-1.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261a439c-6adf-45be-9331-cd699c8449c0",
   "metadata": {},
   "source": [
    "## Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cce05a36-5b73-4cff-9858-da51fcee87e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "from typing import List, Tuple, Optional, Dict\n",
    "\n",
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from scipy import signal\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e42ce8b-f8c7-46db-a50f-afa1ac3226aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Noise generator (your function, slightly hardened)\n",
    "# -------------------------\n",
    "def create_custom_noise_profile(duration, sample_rate, overall_gain_db=-25):\n",
    "    \"\"\"Return a noise array length = duration*sample_rate (dtype=float32).\"\"\"\n",
    "    t = np.linspace(0, duration, int(duration * sample_rate), endpoint=False)\n",
    "    harmonic_noise = np.zeros_like(t)\n",
    "\n",
    "    # harmonic hum\n",
    "    fundamental_freqs = [440, 516, 645]\n",
    "    for fundamental in fundamental_freqs:\n",
    "        for harmonic in range(2, 7):\n",
    "            freq = fundamental * harmonic\n",
    "            detuned_freq = freq * (1 + np.random.uniform(-0.01, 0.01))\n",
    "            amplitude = 0.15 / harmonic\n",
    "            am_depth = 0.1\n",
    "            am_rate = 0.5\n",
    "            am_mod = 1 + am_depth * np.sin(2 * np.pi * am_rate * t)\n",
    "            harmonic_noise += amplitude * am_mod * np.sin(2 * np.pi * detuned_freq * t)\n",
    "\n",
    "    # resonant peaks (narrow band)\n",
    "    resonant_freqs = [3158, 3856, 5109]\n",
    "    resonant_amplitudes = [0.08, 0.08, 0.06]\n",
    "    resonant_noise = np.zeros_like(t)\n",
    "    nyquist = sample_rate / 2.0\n",
    "    for freq, amp in zip(resonant_freqs, resonant_amplitudes):\n",
    "        white = np.random.normal(0, 1, len(t))\n",
    "        low_cut = max(0.0001, (freq * 0.9) / nyquist)\n",
    "        high_cut = min(0.9999, (freq * 1.1) / nyquist)\n",
    "        try:\n",
    "            b, a = signal.butter(4, [low_cut, high_cut], btype='band')\n",
    "            narrow = signal.filtfilt(b, a, white)\n",
    "        except Exception:\n",
    "            narrow = white\n",
    "        resonant_noise += amp * narrow\n",
    "\n",
    "    # broadband hiss shaped by FIR\n",
    "    white_noise = np.random.normal(0, 1, len(t))\n",
    "    try:\n",
    "        from scipy.signal import firwin2\n",
    "        freq_points = [0, 1000, 1290, 1548, 1858, 2229, 2675, 4000, 6000, sample_rate/2]\n",
    "        gain_response = [10, 15, 15, 15, 15, 15, 15, 8, 5, 5]\n",
    "        norm = np.array(freq_points) / (sample_rate/2)\n",
    "        norm = np.clip(norm, 0.0, 1.0)\n",
    "        fir_coeffs = firwin2(1025, norm, gain_response)\n",
    "        shaped_hiss = signal.filtfilt(fir_coeffs, [1.0], white_noise)\n",
    "    except Exception:\n",
    "        shaped_hiss = white_noise\n",
    "\n",
    "    # combine -> apply notch -> normalize -> gain\n",
    "    combined = harmonic_noise + resonant_noise + shaped_hiss\n",
    "    try:\n",
    "        b_notch, a_notch = signal.iirnotch(3179.3, 4, sample_rate)\n",
    "        combined = signal.filtfilt(b_notch, a_notch, combined)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    maxabs = np.max(np.abs(combined)) + 1e-12\n",
    "    combined = combined / maxabs\n",
    "    gain_lin = 10 ** (overall_gain_db / 20.0)\n",
    "    return (combined * gain_lin).astype(np.float32)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# SAD: find non-silent intervals (librosa)\n",
    "# -------------------------\n",
    "def detect_activity_intervals(audio: np.ndarray, sr: int, top_db: float = 30.0, frame_length: int = 2048, hop_length: int = 512) -> List[Tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    Return list of (start_sample, end_sample) intervals containing activity.\n",
    "    Uses librosa.effects.split which is a simple SAD (energy thresholding).\n",
    "    top_db: threshold in dB below reference to consider silence (lower -> more aggressive keep)\n",
    "    \"\"\"\n",
    "    intervals = librosa.effects.split(y=audio, top_db=top_db, frame_length=frame_length, hop_length=hop_length)\n",
    "    return [(int(s), int(e)) for s, e in intervals]\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Helper: sample one fixed-length clip from non-silent intervals\n",
    "# -------------------------\n",
    "def sample_clip_from_intervals(audio: np.ndarray, sr: int, intervals: List[Tuple[int,int]], clip_duration: float, rng: Optional[random.Random] = None) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Choose a random interval that can contain a clip of clip_duration.\n",
    "    If no single interval is long enough, try to stitch or center-pad shorter audio to clip length.\n",
    "    Returns a numpy array of length = clip_duration * sr\n",
    "    \"\"\"\n",
    "    rng = rng or random\n",
    "    clip_len = int(round(clip_duration * sr))\n",
    "    # Filter intervals long enough\n",
    "    long_intervals = [iv for iv in intervals if (iv[1] - iv[0]) >= clip_len]\n",
    "    if long_intervals:\n",
    "        s, e = rng.choice(long_intervals)\n",
    "        start = rng.randint(s, e - clip_len)\n",
    "        clip = audio[start:start + clip_len]\n",
    "        return clip.astype(np.float32)\n",
    "    # otherwise try to sample from any interval, possibly concatenating up to clip_len by wrapping/padding:\n",
    "    if intervals:\n",
    "        # pick a random interval, extract it, then either pad or loop to reach clip_len\n",
    "        s, e = rng.choice(intervals)\n",
    "        seg = audio[s:e].astype(np.float32)\n",
    "        if len(seg) >= clip_len:\n",
    "            # deterministic crop\n",
    "            start = rng.randint(0, len(seg) - clip_len)\n",
    "            return seg[start:start+clip_len]\n",
    "        else:\n",
    "            # repeat or pad center\n",
    "            needed = clip_len - len(seg)\n",
    "            left = needed // 2\n",
    "            right = needed - left\n",
    "            return np.pad(seg, (left, right), mode='constant', constant_values=0.0)\n",
    "    # if no intervals (silent file) -> zero pad or use entire audio center\n",
    "    if len(audio) >= clip_len:\n",
    "        center = len(audio) // 2\n",
    "        start = max(0, center - clip_len // 2)\n",
    "        return audio[start:start + clip_len].astype(np.float32)\n",
    "    else:\n",
    "        return np.pad(audio.astype(np.float32), (0, clip_len - len(audio)), mode='constant')\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# RMS utilities\n",
    "# -------------------------\n",
    "def rms(x: np.ndarray, eps=1e-12) -> float:\n",
    "    return float(np.sqrt(np.mean(x.astype(np.float64)**2) + eps))\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Build one simulated mixture example (core of augmentation)\n",
    "# -------------------------\n",
    "def build_simulated_mixture(\n",
    "    stem_paths: List[Path],\n",
    "    sr: int,\n",
    "    clip_duration: float = 3.0,\n",
    "    min_stems: int = 1,\n",
    "    max_stems: int = 8,\n",
    "    energy_db_range: Tuple[float, float] = (-10.0, 10.0),\n",
    "    rng: Optional[random.Random] = None,\n",
    "    top_db: float = 30.0\n",
    ") -> Tuple[np.ndarray, List[Dict]]:\n",
    "    \"\"\"\n",
    "    Given a list of available stems (Paths), randomly select k stems (k in [min_stems, max_stems])\n",
    "    and produce a clean mixture (1D numpy array of length clip_duration*sr) and metadata list.\n",
    "    Metadata describes which stems, start samples, applied dB gains, original RMS.\n",
    "    \"\"\"\n",
    "    rng = rng or random\n",
    "    n_available = len(stem_paths)\n",
    "    if n_available == 0:\n",
    "        raise ValueError(\"No stems provided to build_simulated_mixture()\")\n",
    "\n",
    "    k = rng.randint(min_stems, min(max_stems, n_available))\n",
    "    selected = rng.sample(stem_paths, k)\n",
    "    clip_len = int(round(clip_duration * sr))\n",
    "\n",
    "    mixture = np.zeros(clip_len, dtype=np.float32)\n",
    "    metadata = []\n",
    "\n",
    "    for p in selected:\n",
    "        audio, file_sr = librosa.load(str(p), sr=None, mono=True)\n",
    "        # resample if needed\n",
    "        if file_sr != sr:\n",
    "            audio = librosa.resample(audio, orig_sr=file_sr, target_sr=sr)\n",
    "        intervals = detect_activity_intervals(audio, sr, top_db=top_db)\n",
    "        clip = sample_clip_from_intervals(audio, sr, intervals, clip_duration, rng=rng)\n",
    "        orig_rms = rms(clip)\n",
    "        db_change = rng.uniform(energy_db_range[0], energy_db_range[1])\n",
    "        gain_lin = 10 ** (db_change / 20.0)\n",
    "        scaled = (clip * gain_lin).astype(np.float32)\n",
    "        # sum to mixture\n",
    "        mixture = mixture + scaled\n",
    "        metadata.append({\n",
    "            \"stem_path\": str(p),\n",
    "            \"db_change\": float(db_change),\n",
    "            \"gain_lin\": float(gain_lin),\n",
    "            \"orig_rms\": float(orig_rms)\n",
    "        })\n",
    "\n",
    "    # After summing, avoid clipping: scale mixture by peak if needed, but preserve RMS relationships.\n",
    "    peak = float(np.max(np.abs(mixture)) + 1e-12)\n",
    "    if peak > 0.99:\n",
    "        mixture = (mixture / peak * 0.99).astype(np.float32)\n",
    "\n",
    "    return mixture, metadata\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Add synthetic noise to get input features + return metadata\n",
    "# -------------------------\n",
    "def add_noise_to_mixture(\n",
    "    clean_mixture: np.ndarray,\n",
    "    sr: int,\n",
    "    snr_db: float,\n",
    "    noise_func=create_custom_noise_profile,\n",
    "    overall_noise_gain_db: float = -25.0\n",
    ") -> Tuple[np.ndarray, Dict]:\n",
    "    \"\"\"\n",
    "    Create noise using noise_func, scale to target SNR with respect to clean_mixture RMS,\n",
    "    return noisy_mixture and noise metadata.\n",
    "    \"\"\"\n",
    "    duration = len(clean_mixture) / sr\n",
    "    noise = noise_func(duration, sr, overall_gain_db=overall_noise_gain_db)\n",
    "    if len(noise) > len(clean_mixture):\n",
    "        noise = noise[:len(clean_mixture)]\n",
    "    elif len(noise) < len(clean_mixture):\n",
    "        noise = np.pad(noise, (0, len(clean_mixture)-len(noise)))\n",
    "\n",
    "    rms_clean = rms(clean_mixture)\n",
    "    rms_noise = rms(noise)\n",
    "    target_lin = 10 ** (snr_db / 20.0)\n",
    "    required_noise_rms = (rms_clean / target_lin) if target_lin > 0 else rms_clean\n",
    "    noise_gain = (required_noise_rms / (rms_noise + 1e-12))\n",
    "    adjusted_noise = (noise * noise_gain).astype(np.float32)\n",
    "    noisy = clean_mixture + adjusted_noise\n",
    "\n",
    "    # prevent clipping\n",
    "    peak = float(np.max(np.abs(noisy)) + 1e-12)\n",
    "    if peak > 1.0:\n",
    "        noisy = (noisy / peak * 0.99).astype(np.float32)\n",
    "\n",
    "    meta = {\n",
    "        \"snr_db_target\": float(snr_db),\n",
    "        \"rms_clean\": float(rms_clean),\n",
    "        \"rms_noise_before_gain\": float(rms_noise),\n",
    "        \"noise_gain\": float(noise_gain),\n",
    "        \"overall_noise_profile_db\": float(overall_noise_gain_db)\n",
    "    }\n",
    "    return noisy, meta\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Pipeline orchestration (Airflow friendly)\n",
    "# -------------------------\n",
    "def run_augmentation_pipeline(\n",
    "    stems_root: str,\n",
    "    output_base: str = \"dataset\",\n",
    "    dataset_name: str = \"aug_mixtures_v1\",\n",
    "    sample_rate: int = 22050,\n",
    "    clip_duration: float = 3.0,\n",
    "    min_stems: int = 1,\n",
    "    max_stems: int = 8,\n",
    "    energy_db_range: Tuple[float,float] = (-10.0, 10.0),\n",
    "    snr_db_range: Tuple[float,float] = (5.0, 20.0),\n",
    "    max_files: Optional[int] = None,         # restrict number of stems considered (N); None -> all\n",
    "    n_examples: int = 1000,                  # number of augmented examples to synthesize\n",
    "    top_db: float = 30.0,\n",
    "    seed: Optional[int] = None\n",
    "):\n",
    "    \"\"\"\n",
    "    High-level pipeline:\n",
    "      - discover stems (N)\n",
    "      - limit to max_files if provided\n",
    "      - for i in [0..n_examples): create one mixture example:\n",
    "          - randomly choose between min_stems..max_stems stems\n",
    "          - sample clip_duration from each stem (using SAD)\n",
    "          - apply dB scaling per stem\n",
    "          - sum -> clean_mixture (target)\n",
    "          - sample snr in snr_db_range -> create noisy (feature)\n",
    "          - save noisy and clean wavs + metadata json\n",
    "    \"\"\"\n",
    "    rng = random.Random(seed)\n",
    "    stems_root = Path(stems_root)\n",
    "    stem_paths = sorted([p for p in stems_root.rglob(\"*\") if p.suffix.lower() in (\".wav\", \".flac\", \".mp3\")])\n",
    "    if not stem_paths:\n",
    "        raise RuntimeError(f\"No audio stems found in {stems_root}\")\n",
    "\n",
    "    if max_files is not None:\n",
    "        stem_paths = stem_paths[:max_files]\n",
    "\n",
    "    out_root = Path(output_base) / \"processed\" / dataset_name\n",
    "    clean_dir = out_root / \"clean\"\n",
    "    noisy_dir = out_root / \"noisy\"\n",
    "    meta_dir = out_root / \"meta\"\n",
    "    clean_dir.mkdir(parents=True, exist_ok=True)\n",
    "    noisy_dir.mkdir(parents=True, exist_ok=True)\n",
    "    meta_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(f\"Found {len(stem_paths)} stems. Will create {n_examples} examples using up to {max_stems} stems each.\")\n",
    "\n",
    "    for idx in tqdm(range(n_examples), desc=\"Synth examples\"):\n",
    "        # Build clean mixture\n",
    "        mixture, stems_meta = build_simulated_mixture(\n",
    "            stem_paths=stem_paths,\n",
    "            sr=sample_rate,\n",
    "            clip_duration=clip_duration,\n",
    "            min_stems=min_stems,\n",
    "            max_stems=max_stems,\n",
    "            energy_db_range=energy_db_range,\n",
    "            rng=rng,\n",
    "            top_db=top_db\n",
    "        )\n",
    "\n",
    "        # Choose an SNR for noise\n",
    "        snr = float(rng.uniform(snr_db_range[0], snr_db_range[1]))\n",
    "        noisy, noise_meta = add_noise_to_mixture(mixture, sr=sample_rate, snr_db=snr)\n",
    "\n",
    "        # Save files\n",
    "        basename = f\"example_{idx:06d}\"\n",
    "        clean_path = clean_dir / f\"{basename}_clean.wav\"\n",
    "        noisy_path = noisy_dir / f\"{basename}_noisy.wav\"\n",
    "        meta_path = meta_dir / f\"{basename}.json\"\n",
    "\n",
    "        # sf.write(str(clean_path), mixture, sample_rate)\n",
    "        # sf.write(str(noisy_path), noisy, sample_rate)\n",
    "\n",
    "        # Derived components\n",
    "        target = mixture                      # clean signal\n",
    "        residual = noisy - mixture             # noise component\n",
    "\n",
    "        target_path = clean_dir / f\"{basename}_target.wav\"\n",
    "        residual_path = clean_dir / f\"{basename}_residual.wav\"\n",
    "        mix_path = noisy_dir / f\"{basename}_mix.wav\"\n",
    "\n",
    "        sf.write(str(target_path), target, sample_rate)\n",
    "        sf.write(str(residual_path), residual, sample_rate)\n",
    "        sf.write(str(mix_path), noisy, sample_rate)\n",
    "\n",
    "        full_meta = {\n",
    "            \"example_id\": basename,\n",
    "            \"sample_rate\": int(sample_rate),\n",
    "            \"clip_duration\": float(clip_duration),\n",
    "            \"chosen_snr_db\": float(snr),\n",
    "            \"paths\": {\n",
    "                \"target\": str(target_path),\n",
    "                \"residual\": str(residual_path),\n",
    "                \"mix\": str(mix_path),\n",
    "            },\n",
    "            \"stems_meta\": stems_meta,\n",
    "            \"noise_meta\": noise_meta\n",
    "        }\n",
    "\n",
    "        with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(full_meta, f, indent=2)\n",
    "\n",
    "    print(f\"Saved {n_examples} examples to {out_root}\")\n",
    "    return str(out_root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fb107d1-1428-4f29-87fb-83d4376c631a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 261 stems. Will create 200 examples using up to 8 stems each.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Synth examples:   0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Synth examples: 100%|██████████| 200/200 [01:59<00:00,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 200 examples to guitar_dataset\\processed\\dataset2-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'guitar_dataset\\\\processed\\\\dataset2-v2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dir = \"IDMT-SMT-GUITAR_V2/dataset2/audio/\"\n",
    "\n",
    "run_augmentation_pipeline(\n",
    "    stems_root=input_dir,\n",
    "    output_base=\"guitar_dataset\",\n",
    "    dataset_name=\"dataset2-v2\",\n",
    "    n_examples=200\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f77fe1",
   "metadata": {},
   "source": [
    "## Model Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "daecaff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "model_pipeline.py\n",
    "\n",
    "Airflow-friendly model engineering module:\n",
    "- load configs\n",
    "- dataset creation (clean/noisy pairs)\n",
    "- model build (Wave-U-Net 1D)\n",
    "- train_epoch / validate_epoch\n",
    "- checkpointing, plotting\n",
    "- optional MLflow logging\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import logging\n",
    "from typing import Tuple, Dict, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "a6259584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional YAML support\n",
    "try:\n",
    "    import yaml\n",
    "except Exception:\n",
    "    yaml = None\n",
    "\n",
    "# Optional MLflow\n",
    "try:\n",
    "    import mlflow\n",
    "    _mlflow_available = True\n",
    "except Exception:\n",
    "    mlflow = None\n",
    "    _mlflow_available = False\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"model_pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "a18e759c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 1) Config loading utility\n",
    "# ---------------------------\n",
    "def load_configs(train_config_path: str, metrics_config_path: str) -> Tuple[dict, dict]:\n",
    "    \"\"\"\n",
    "    Load training configuration and metrics/plot configuration from YAML or JSON.\n",
    "    Returns (train_cfg, metrics_cfg)\n",
    "    \"\"\"\n",
    "    def _load(path: str):\n",
    "        p = Path(path)\n",
    "        if not p.exists():\n",
    "            raise FileNotFoundError(f\"Config file not found: {path}\")\n",
    "        text = p.read_text(encoding=\"utf-8\")\n",
    "        if yaml and (p.suffix.lower() in (\".yml\", \".yaml\")):\n",
    "            return yaml.safe_load(text)\n",
    "        # try json\n",
    "        try:\n",
    "            return json.loads(text)\n",
    "        except Exception:\n",
    "            if yaml:\n",
    "                return yaml.safe_load(text)\n",
    "            raise\n",
    "\n",
    "    train_cfg = _load(train_config_path)\n",
    "    metrics_cfg = _load(metrics_config_path)\n",
    "    return train_cfg, metrics_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "da2b2f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 2) Dataset for target/residual/mix triplets\n",
    "# ---------------------------\n",
    "class WaveUNetDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Loads triplets from processed dataset directory:\n",
    "      processed/{dataset_name}/clean/*_target.wav\n",
    "      processed/{dataset_name}/clean/*_residual.wav\n",
    "      processed/{dataset_name}/noisy/*_mix.wav\n",
    "\n",
    "    It matches by basename prefix (e.g. example_000001_target.wav / example_000001_residual.wav / example_000001_mix.wav)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, processed_root: str, dataset_name: str, split: str = \"train\"):\n",
    "        \"\"\"\n",
    "        processed_root: base path (e.g., datasets/processed)\n",
    "        dataset_name: the dataset folder name\n",
    "        split: 'train' or 'test' - supports:\n",
    "            - processed/{dataset_name}/clean + /noisy\n",
    "            - processed/{dataset_name}/{split}/clean + /noisy\n",
    "        \"\"\"\n",
    "        base = Path(processed_root) / dataset_name\n",
    "        if not base.exists():\n",
    "            raise RuntimeError(f\"Processed dataset not found: {base}\")\n",
    "\n",
    "        # possible directory layouts\n",
    "        possible_clean = base / \"clean\"\n",
    "        possible_noisy = base / \"noisy\"\n",
    "        alt_clean = base / split / \"clean\"\n",
    "        alt_noisy = base / split / \"noisy\"\n",
    "\n",
    "        if alt_clean.exists() and alt_noisy.exists():\n",
    "            self.clean_dir = alt_clean\n",
    "            self.noisy_dir = alt_noisy\n",
    "        elif possible_clean.exists() and possible_noisy.exists():\n",
    "            self.clean_dir = possible_clean\n",
    "            self.noisy_dir = possible_noisy\n",
    "        else:\n",
    "            raise RuntimeError(f\"Could not find clean/noisy directories in {base} or {base}/{split}\")\n",
    "\n",
    "        # list all relevant files\n",
    "        target_files = sorted(self.clean_dir.glob(\"*_target.wav\"))\n",
    "        residual_files = sorted(self.clean_dir.glob(\"*_residual.wav\"))\n",
    "        mix_files = sorted(self.noisy_dir.glob(\"*_mix.wav\"))\n",
    "\n",
    "        # build basename maps\n",
    "        def key_from_path(p: Path):\n",
    "            stem = p.stem\n",
    "            for s in (\"_target\", \"_residual\", \"_mix\"):\n",
    "                if stem.endswith(s):\n",
    "                    return stem[: -len(s)]\n",
    "            return stem\n",
    "\n",
    "        target_map = {key_from_path(p): p for p in target_files}\n",
    "        residual_map = {key_from_path(p): p for p in residual_files}\n",
    "        mix_map = {key_from_path(p): p for p in mix_files}\n",
    "\n",
    "        # intersection of all three\n",
    "        keys = sorted(list(set(target_map.keys()) & set(residual_map.keys()) & set(mix_map.keys())))\n",
    "        if not keys:\n",
    "            raise RuntimeError(f\"No matching target/residual/mix triplets found in {self.clean_dir} and {self.noisy_dir}\")\n",
    "\n",
    "        self.triplets = [(target_map[k], residual_map[k], mix_map[k]) for k in keys]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.triplets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        target_p, residual_p, mix_p = self.triplets[idx]\n",
    "\n",
    "        # load\n",
    "        target, sr1 = sf.read(str(target_p))\n",
    "        residual, sr2 = sf.read(str(residual_p))\n",
    "        mix, sr3 = sf.read(str(mix_p))\n",
    "\n",
    "        # ensure mono + same sr\n",
    "        if target.ndim > 1:\n",
    "            target = np.mean(target, axis=1)\n",
    "        if residual.ndim > 1:\n",
    "            residual = np.mean(residual, axis=1)\n",
    "        if mix.ndim > 1:\n",
    "            mix = np.mean(mix, axis=1)\n",
    "        if not (sr1 == sr2 == sr3):\n",
    "            raise RuntimeError(f\"Sample rates mismatch for example {target_p.stem}\")\n",
    "\n",
    "        # make sure lengths align\n",
    "        min_len = min(len(target), len(residual), len(mix))\n",
    "        target = target[:min_len]\n",
    "        residual = residual[:min_len]\n",
    "        mix = mix[:min_len]\n",
    "\n",
    "        # convert to (1, L) tensors\n",
    "        target = torch.from_numpy(target.astype(np.float32)).unsqueeze(0)\n",
    "        residual = torch.from_numpy(residual.astype(np.float32)).unsqueeze(0)\n",
    "        mix = torch.from_numpy(mix.astype(np.float32)).unsqueeze(0)\n",
    "\n",
    "        return mix, target, residual  # input, clean target, noise residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "64302b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 3) Splitting helper (for target/residual/mix dataset)\n",
    "# ---------------------------\n",
    "def create_train_test_splits(processed_root: str, dataset_name: str, train_frac: float = 0.9, seed: int = 42):\n",
    "    \"\"\"\n",
    "    Creates train/test subfolders for WaveUNetDataset-style processed datasets.\n",
    "    Works with:\n",
    "      processed/{dataset_name}/clean/*_target.wav\n",
    "      processed/{dataset_name}/clean/*_residual.wav\n",
    "      processed/{dataset_name}/noisy/*_mix.wav\n",
    "\n",
    "    Output:\n",
    "      processed/{dataset_name}/train/{clean,noisy}\n",
    "      processed/{dataset_name}/test/{clean,noisy}\n",
    "\n",
    "    Returns:\n",
    "      (train_dataset_dir, test_dataset_dir)\n",
    "    \"\"\"\n",
    "    base = Path(processed_root) / dataset_name\n",
    "    train_clean = base / \"train\" / \"clean\"\n",
    "    train_noisy = base / \"train\" / \"noisy\"\n",
    "    test_clean = base / \"test\" / \"clean\"\n",
    "    test_noisy = base / \"test\" / \"noisy\"\n",
    "\n",
    "    # if already split, skip\n",
    "    if train_clean.exists() and test_clean.exists():\n",
    "        print(\"Train/test subfolders already exist; skipping split creation.\")\n",
    "        return str(base), str(base)\n",
    "\n",
    "    # locate files\n",
    "    all_target = sorted((base / \"clean\").glob(\"*_target.wav\"))\n",
    "    all_residual = sorted((base / \"clean\").glob(\"*_residual.wav\"))\n",
    "    all_mix = sorted((base / \"noisy\").glob(\"*_mix.wav\"))\n",
    "\n",
    "    if not all_target or not all_residual or not all_mix:\n",
    "        raise RuntimeError(f\"Missing one or more sets of files in {base} (need target, residual, mix)\")\n",
    "\n",
    "    # match keys like WaveUNetDataset\n",
    "    def key(p: Path):\n",
    "        s = p.stem\n",
    "        for suf in (\"_target\", \"_residual\", \"_mix\"):\n",
    "            if s.endswith(suf):\n",
    "                return s[:-len(suf)]\n",
    "        return s\n",
    "\n",
    "    target_map = {key(p): p for p in all_target}\n",
    "    residual_map = {key(p): p for p in all_residual}\n",
    "    mix_map = {key(p): p for p in all_mix}\n",
    "\n",
    "    keys = sorted(list(set(target_map.keys()) & set(residual_map.keys()) & set(mix_map.keys())))\n",
    "    if not keys:\n",
    "        raise RuntimeError(f\"No matching triplets found under {base}\")\n",
    "\n",
    "    # random split\n",
    "    rng = random.Random(seed)\n",
    "    rng.shuffle(keys)\n",
    "    n_train = int(math.floor(len(keys) * train_frac))\n",
    "    train_keys = keys[:n_train]\n",
    "    test_keys = keys[n_train:]\n",
    "\n",
    "    # make dirs\n",
    "    for d in [train_clean, train_noisy, test_clean, test_noisy]:\n",
    "        d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    from shutil import copy2\n",
    "    def copy_triplet(k: str, dst_clean: Path, dst_noisy: Path):\n",
    "        copy2(target_map[k], dst_clean / target_map[k].name)\n",
    "        copy2(residual_map[k], dst_clean / residual_map[k].name)\n",
    "        copy2(mix_map[k], dst_noisy / mix_map[k].name)\n",
    "\n",
    "    for k in train_keys:\n",
    "        copy_triplet(k, train_clean, train_noisy)\n",
    "    for k in test_keys:\n",
    "        copy_triplet(k, test_clean, test_noisy)\n",
    "\n",
    "    print(f\"Created train/test split: {len(train_keys)} train, {len(test_keys)} test examples.\")\n",
    "    return str(base), str(base)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "aeb565a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 5) Wave-UNET Architecture\n",
    "# ---------------------------\n",
    "\n",
    "# conv.py\n",
    "\n",
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, conv_type, transpose=False):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        self.transpose = transpose\n",
    "        self.stride = stride\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv_type = conv_type\n",
    "\n",
    "        # How many channels should be normalised as one group if GroupNorm is activated\n",
    "        # WARNING: Number of channels has to be divisible by this number!\n",
    "        NORM_CHANNELS = 8\n",
    "\n",
    "        if self.transpose:\n",
    "            self.filter = nn.ConvTranspose1d(n_inputs, n_outputs, self.kernel_size, stride, padding=kernel_size-1)\n",
    "        else:\n",
    "            self.filter = nn.Conv1d(n_inputs, n_outputs, self.kernel_size, stride)\n",
    "\n",
    "        if conv_type == \"gn\":\n",
    "            assert(n_outputs % NORM_CHANNELS == 0)\n",
    "            self.norm = nn.GroupNorm(n_outputs // NORM_CHANNELS, n_outputs)\n",
    "        elif conv_type == \"bn\":\n",
    "            self.norm = nn.BatchNorm1d(n_outputs, momentum=0.01)\n",
    "        # Add you own types of variations here!\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the convolution\n",
    "        if self.conv_type == \"gn\" or self.conv_type == \"bn\":\n",
    "            out = nn.functional.relu(self.norm((self.filter(x))))\n",
    "        else: # Add your own variations here with elifs conditioned on \"conv_type\" parameter!\n",
    "            assert(self.conv_type == \"normal\")\n",
    "            out =nn.functional.leaky_relu(self.filter(x))\n",
    "        return out\n",
    "\n",
    "    def get_input_size(self, output_size):\n",
    "        # Strided conv/decimation\n",
    "        if not self.transpose:\n",
    "            curr_size = (output_size - 1)*self.stride + 1 # o = (i-1)//s + 1 => i = (o - 1)*s + 1\n",
    "        else:\n",
    "            curr_size = output_size\n",
    "\n",
    "        # Conv\n",
    "        curr_size = curr_size + self.kernel_size - 1 # o = i + p - k + 1\n",
    "\n",
    "        # Transposed\n",
    "        if self.transpose:\n",
    "            assert ((curr_size - 1) % self.stride == 0)# We need to have a value at the beginning and end\n",
    "            curr_size = ((curr_size - 1) // self.stride) + 1\n",
    "        assert(curr_size > 0)\n",
    "        return curr_size\n",
    "\n",
    "    def get_output_size(self, input_size):\n",
    "        # Transposed\n",
    "        if self.transpose:\n",
    "            assert(input_size > 1)\n",
    "            curr_size = (input_size - 1)*self.stride + 1 # o = (i-1)//s + 1 => i = (o - 1)*s + 1\n",
    "        else:\n",
    "            curr_size = input_size\n",
    "\n",
    "        # Conv\n",
    "        curr_size = curr_size - self.kernel_size + 1 # o = i + p - k + 1\n",
    "        assert (curr_size > 0)\n",
    "\n",
    "        # Strided conv/decimation\n",
    "        if not self.transpose:\n",
    "            assert ((curr_size - 1) % self.stride == 0)  # We need to have a value at the beginning and end\n",
    "            curr_size = ((curr_size - 1) // self.stride) + 1\n",
    "\n",
    "        return curr_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "de8c09e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crop.py\n",
    "\n",
    "def centre_crop(x, target):\n",
    "    '''\n",
    "    Center-crop 3-dim. input tensor along last axis so it fits the target tensor shape\n",
    "    :param x: Input tensor\n",
    "    :param target: Shape of this tensor will be used as target shape\n",
    "    :return: Cropped input tensor\n",
    "    '''\n",
    "    if x is None:\n",
    "        return None\n",
    "    if target is None:\n",
    "        return x\n",
    "\n",
    "    target_shape = target.shape\n",
    "    diff = x.shape[-1] - target_shape[-1]\n",
    "    assert (diff % 2 == 0)\n",
    "    crop = diff // 2\n",
    "\n",
    "    if crop == 0:\n",
    "        return x\n",
    "    if crop < 0:\n",
    "        raise ArithmeticError\n",
    "\n",
    "    return x[:, :, crop:-crop].contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "44c23fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample.py\n",
    "\n",
    "class Resample1d(nn.Module):\n",
    "    def __init__(self, channels, kernel_size, stride, transpose=False, padding=\"reflect\", trainable=False):\n",
    "        '''\n",
    "        Creates a resampling layer for time series data (using 1D convolution) - (N, C, W) input format\n",
    "        :param channels: Number of features C at each time-step\n",
    "        :param kernel_size: Width of sinc-based lowpass-filter (>= 15 recommended for good filtering performance)\n",
    "        :param stride: Resampling factor (integer)\n",
    "        :param transpose: False for down-, true for upsampling\n",
    "        :param padding: Either \"reflect\" to pad or \"valid\" to not pad\n",
    "        :param trainable: Optionally activate this to train the lowpass-filter, starting from the sinc initialisation\n",
    "        '''\n",
    "        super(Resample1d, self).__init__()\n",
    "\n",
    "        self.padding = padding\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.transpose = transpose\n",
    "        self.channels = channels\n",
    "\n",
    "        cutoff = 0.5 / stride\n",
    "\n",
    "        assert(kernel_size > 2)\n",
    "        assert ((kernel_size - 1) % 2 == 0)\n",
    "        assert(padding == \"reflect\" or padding == \"valid\")\n",
    "\n",
    "        filter = build_sinc_filter(kernel_size, cutoff)\n",
    "\n",
    "        self.filter = torch.nn.Parameter(torch.from_numpy(np.repeat(np.reshape(filter, [1, 1, kernel_size]), channels, axis=0)), requires_grad=trainable)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pad here if not using transposed conv\n",
    "        input_size = x.shape[2]\n",
    "        if self.padding != \"valid\":\n",
    "            num_pad = (self.kernel_size-1)//2\n",
    "            out = nn.functional.pad(x, (num_pad, num_pad), mode=self.padding)\n",
    "        else:\n",
    "            out = x\n",
    "\n",
    "        # Lowpass filter (+ 0 insertion if transposed)\n",
    "        if self.transpose:\n",
    "            expected_steps = ((input_size - 1) * self.stride + 1)\n",
    "            if self.padding == \"valid\":\n",
    "                expected_steps = expected_steps - self.kernel_size + 1\n",
    "\n",
    "            out = nn.functional.conv_transpose1d(out, self.filter, stride=self.stride, padding=0, groups=self.channels)\n",
    "            diff_steps = out.shape[2] - expected_steps\n",
    "            if diff_steps > 0:\n",
    "                assert(diff_steps % 2 == 0)\n",
    "                out = out[:,:,diff_steps//2:-diff_steps//2]\n",
    "        else:\n",
    "            assert(input_size % self.stride == 1)\n",
    "            out = nn.functional.conv1d(out, self.filter, stride=self.stride, padding=0, groups=self.channels)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def get_output_size(self, input_size):\n",
    "        '''\n",
    "        Returns the output dimensionality (number of timesteps) for a given input size\n",
    "        :param input_size: Number of input time steps (Scalar, each feature is one-dimensional)\n",
    "        :return: Output size (scalar)\n",
    "        '''\n",
    "        assert(input_size > 1)\n",
    "        if self.transpose:\n",
    "            if self.padding == \"valid\":\n",
    "                return ((input_size - 1) * self.stride + 1) - self.kernel_size + 1\n",
    "            else:\n",
    "                return ((input_size - 1) * self.stride + 1)\n",
    "        else:\n",
    "            assert(input_size % self.stride == 1) # Want to take first and last sample\n",
    "            if self.padding == \"valid\":\n",
    "                return input_size - self.kernel_size + 1\n",
    "            else:\n",
    "                return input_size\n",
    "\n",
    "    def get_input_size(self, output_size):\n",
    "        '''\n",
    "        Returns the input dimensionality (number of timesteps) for a given output size\n",
    "        :param input_size: Number of input time steps (Scalar, each feature is one-dimensional)\n",
    "        :return: Output size (scalar)\n",
    "        '''\n",
    "\n",
    "        # Strided conv/decimation\n",
    "        if not self.transpose:\n",
    "            curr_size = (output_size - 1)*self.stride + 1 # o = (i-1)//s + 1 => i = (o - 1)*s + 1\n",
    "        else:\n",
    "            curr_size = output_size\n",
    "\n",
    "        # Conv\n",
    "        if self.padding == \"valid\":\n",
    "            curr_size = curr_size + self.kernel_size - 1 # o = i + p - k + 1\n",
    "\n",
    "        # Transposed\n",
    "        if self.transpose:\n",
    "            assert ((curr_size - 1) % self.stride == 0)# We need to have a value at the beginning and end\n",
    "            curr_size = ((curr_size - 1) // self.stride) + 1\n",
    "        assert(curr_size > 0)\n",
    "        return curr_size\n",
    "\n",
    "def build_sinc_filter(kernel_size, cutoff):\n",
    "    # FOLLOWING https://www.analog.com/media/en/technical-documentation/dsp-book/dsp_book_Ch16.pdf\n",
    "    # Sinc lowpass filter\n",
    "    # Build sinc kernel\n",
    "    assert(kernel_size % 2 == 1)\n",
    "    M = kernel_size - 1\n",
    "    filter = np.zeros(kernel_size, dtype=np.float32)\n",
    "    for i in range(kernel_size):\n",
    "        if i == M//2:\n",
    "            filter[i] = 2 * np.pi * cutoff\n",
    "        else:\n",
    "            filter[i] = (np.sin(2 * np.pi * cutoff * (i - M//2)) / (i - M//2)) * \\\n",
    "                    (0.42 - 0.5 * np.cos((2 * np.pi * i) / M) + 0.08 * np.cos(4 * np.pi * M))\n",
    "\n",
    "    filter = filter / np.sum(filter)\n",
    "    return filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "75da87df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.py\n",
    "\n",
    "def save_model(model, optimizer, state, path):\n",
    "    if isinstance(model, torch.nn.DataParallel):\n",
    "        model = model.module  # save state dict of wrapped module\n",
    "    if len(os.path.dirname(path)) > 0 and not os.path.exists(os.path.dirname(path)):\n",
    "        os.makedirs(os.path.dirname(path))\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'state': state,  # state of training loop (was 'step')\n",
    "    }, path)\n",
    "\n",
    "\n",
    "def load_model(model, optimizer, path, cuda):\n",
    "    if isinstance(model, torch.nn.DataParallel):\n",
    "        model = model.module  # load state dict of wrapped module\n",
    "    if cuda:\n",
    "        checkpoint = torch.load(path)\n",
    "    else:\n",
    "        checkpoint = torch.load(path, map_location='cpu')\n",
    "    try:\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    except:\n",
    "        # work-around for loading checkpoints where DataParallel was saved instead of inner module\n",
    "        from collections import OrderedDict\n",
    "        model_state_dict_fixed = OrderedDict()\n",
    "        prefix = 'module.'\n",
    "        for k, v in checkpoint['model_state_dict'].items():\n",
    "            if k.startswith(prefix):\n",
    "                k = k[len(prefix):]\n",
    "            model_state_dict_fixed[k] = v\n",
    "        model.load_state_dict(model_state_dict_fixed)\n",
    "    if optimizer is not None:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    if 'state' in checkpoint:\n",
    "        state = checkpoint['state']\n",
    "    else:\n",
    "        # older checkpoints only store step, rest of state won't be there\n",
    "        state = {'step': checkpoint['step']}\n",
    "    return state\n",
    "\n",
    "\n",
    "def compute_loss(model, inputs, targets, criterion, compute_grad=False):\n",
    "    '''\n",
    "    Computes gradients of model with given inputs and targets and loss function.\n",
    "    Optionally backpropagates to compute gradients for weights.\n",
    "    Procedure depends on whether we have one model for each source or not\n",
    "    :param model: Model to train with\n",
    "    :param inputs: Input mixture\n",
    "    :param targets: Target sources\n",
    "    :param criterion: Loss function to use (L1, L2, ..)\n",
    "    :param compute_grad: Whether to compute gradients\n",
    "    :return: Model outputs, Average loss over batch\n",
    "    '''\n",
    "    all_outputs = {}\n",
    "\n",
    "    if model.separate:\n",
    "        avg_loss = 0.0\n",
    "        num_sources = 0\n",
    "        for inst in model.instruments:\n",
    "            output = model(inputs, inst)\n",
    "            loss = criterion(output[inst], targets[inst])\n",
    "\n",
    "            if compute_grad:\n",
    "                loss.backward()\n",
    "\n",
    "            avg_loss += loss.item()\n",
    "            num_sources += 1\n",
    "\n",
    "            all_outputs[inst] = output[inst].detach().clone()\n",
    "\n",
    "        avg_loss /= float(num_sources)\n",
    "    else:\n",
    "        loss = 0\n",
    "        all_outputs = model(inputs)\n",
    "        for inst in all_outputs.keys():\n",
    "            loss += criterion(all_outputs[inst], targets[inst])\n",
    "\n",
    "        if compute_grad:\n",
    "            loss.backward()\n",
    "\n",
    "        avg_loss = loss.item() / float(len(all_outputs))\n",
    "\n",
    "    return all_outputs, avg_loss\n",
    "\n",
    "\n",
    "class DataParallel(torch.nn.DataParallel):\n",
    "    def __init__(self, module, device_ids=None, output_device=None, dim=0):\n",
    "        super(DataParallel, self).__init__(module, device_ids, output_device, dim)\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        try:\n",
    "            return super().__getattr__(name)\n",
    "        except AttributeError:\n",
    "            return getattr(self.module, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "d03aac85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wave_unet.py\n",
    "\n",
    "class UpsamplingBlock(nn.Module):\n",
    "    def __init__(self, n_inputs, n_shortcut, n_outputs, kernel_size, stride, depth, conv_type, res):\n",
    "        super(UpsamplingBlock, self).__init__()\n",
    "        assert(stride > 1)\n",
    "\n",
    "        # CONV 1 for UPSAMPLING\n",
    "        if res == \"fixed\":\n",
    "            self.upconv = Resample1d(n_inputs, 15, stride, transpose=True)\n",
    "        else:\n",
    "            self.upconv = ConvLayer(n_inputs, n_inputs, kernel_size, stride, conv_type, transpose=True)\n",
    "\n",
    "        self.pre_shortcut_convs = nn.ModuleList([ConvLayer(n_inputs, n_outputs, kernel_size, 1, conv_type)] +\n",
    "                                                [ConvLayer(n_outputs, n_outputs, kernel_size, 1, conv_type) for _ in range(depth - 1)])\n",
    "\n",
    "        # CONVS to combine high- with low-level information (from shortcut)\n",
    "        self.post_shortcut_convs = nn.ModuleList([ConvLayer(n_outputs + n_shortcut, n_outputs, kernel_size, 1, conv_type)] +\n",
    "                                                 [ConvLayer(n_outputs, n_outputs, kernel_size, 1, conv_type) for _ in range(depth - 1)])\n",
    "\n",
    "    def forward(self, x, shortcut):\n",
    "        # UPSAMPLE HIGH-LEVEL FEATURES\n",
    "        upsampled = self.upconv(x)\n",
    "\n",
    "        for conv in self.pre_shortcut_convs:\n",
    "            upsampled = conv(upsampled)\n",
    "\n",
    "        # Prepare shortcut connection\n",
    "        combined = centre_crop(shortcut, upsampled)\n",
    "\n",
    "        # Combine high- and low-level features\n",
    "        for conv in self.post_shortcut_convs:\n",
    "            combined = conv(torch.cat([combined, centre_crop(upsampled, combined)], dim=1))\n",
    "        return combined\n",
    "\n",
    "    def get_output_size(self, input_size):\n",
    "        curr_size = self.upconv.get_output_size(input_size)\n",
    "\n",
    "        # Upsampling convs\n",
    "        for conv in self.pre_shortcut_convs:\n",
    "            curr_size = conv.get_output_size(curr_size)\n",
    "\n",
    "        # Combine convolutions\n",
    "        for conv in self.post_shortcut_convs:\n",
    "            curr_size = conv.get_output_size(curr_size)\n",
    "\n",
    "        return curr_size\n",
    "\n",
    "class DownsamplingBlock(nn.Module):\n",
    "    def __init__(self, n_inputs, n_shortcut, n_outputs, kernel_size, stride, depth, conv_type, res):\n",
    "        super(DownsamplingBlock, self).__init__()\n",
    "        assert(stride > 1)\n",
    "\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "\n",
    "        # CONV 1\n",
    "        self.pre_shortcut_convs = nn.ModuleList([ConvLayer(n_inputs, n_shortcut, kernel_size, 1, conv_type)] +\n",
    "                                                [ConvLayer(n_shortcut, n_shortcut, kernel_size, 1, conv_type) for _ in range(depth - 1)])\n",
    "\n",
    "        self.post_shortcut_convs = nn.ModuleList([ConvLayer(n_shortcut, n_outputs, kernel_size, 1, conv_type)] +\n",
    "                                                 [ConvLayer(n_outputs, n_outputs, kernel_size, 1, conv_type) for _ in\n",
    "                                                  range(depth - 1)])\n",
    "\n",
    "        # CONV 2 with decimation\n",
    "        if res == \"fixed\":\n",
    "            self.downconv = Resample1d(n_outputs, 15, stride) # Resampling with fixed-size sinc lowpass filter\n",
    "        else:\n",
    "            self.downconv = ConvLayer(n_outputs, n_outputs, kernel_size, stride, conv_type)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # PREPARING SHORTCUT FEATURES\n",
    "        shortcut = x\n",
    "        for conv in self.pre_shortcut_convs:\n",
    "            shortcut = conv(shortcut)\n",
    "\n",
    "        # PREPARING FOR DOWNSAMPLING\n",
    "        out = shortcut\n",
    "        for conv in self.post_shortcut_convs:\n",
    "            out = conv(out)\n",
    "\n",
    "        # DOWNSAMPLING\n",
    "        out = self.downconv(out)\n",
    "\n",
    "        return out, shortcut\n",
    "\n",
    "    def get_input_size(self, output_size):\n",
    "        curr_size = self.downconv.get_input_size(output_size)\n",
    "\n",
    "        for conv in reversed(self.post_shortcut_convs):\n",
    "            curr_size = conv.get_input_size(curr_size)\n",
    "\n",
    "        for conv in reversed(self.pre_shortcut_convs):\n",
    "            curr_size = conv.get_input_size(curr_size)\n",
    "        return curr_size\n",
    "\n",
    "class Waveunet(nn.Module):\n",
    "    def __init__(self, num_inputs, num_channels, num_outputs, instruments, kernel_size, target_output_size, conv_type, res, separate=False, depth=1, strides=2):\n",
    "        super(Waveunet, self).__init__()\n",
    "\n",
    "        self.num_levels = len(num_channels)\n",
    "        self.strides = strides\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_outputs = num_outputs\n",
    "        self.depth = depth\n",
    "        self.instruments = instruments\n",
    "        self.separate = separate\n",
    "\n",
    "        # Only odd filter kernels allowed\n",
    "        assert(kernel_size % 2 == 1)\n",
    "\n",
    "        self.waveunets = nn.ModuleDict()\n",
    "\n",
    "        model_list = instruments if separate else [\"ALL\"]\n",
    "        # Create a model for each source if we separate sources separately, otherwise only one (model_list=[\"ALL\"])\n",
    "        for instrument in model_list:\n",
    "            module = nn.Module()\n",
    "\n",
    "            module.downsampling_blocks = nn.ModuleList()\n",
    "            module.upsampling_blocks = nn.ModuleList()\n",
    "\n",
    "            for i in range(self.num_levels - 1):\n",
    "                in_ch = num_inputs if i == 0 else num_channels[i]\n",
    "\n",
    "                module.downsampling_blocks.append(\n",
    "                    DownsamplingBlock(in_ch, num_channels[i], num_channels[i+1], kernel_size, strides, depth, conv_type, res))\n",
    "\n",
    "            for i in range(0, self.num_levels - 1):\n",
    "                module.upsampling_blocks.append(\n",
    "                    UpsamplingBlock(num_channels[-1-i], num_channels[-2-i], num_channels[-2-i], kernel_size, strides, depth, conv_type, res))\n",
    "\n",
    "            module.bottlenecks = nn.ModuleList(\n",
    "                [ConvLayer(num_channels[-1], num_channels[-1], kernel_size, 1, conv_type) for _ in range(depth)])\n",
    "\n",
    "            # Output conv\n",
    "            outputs = num_outputs if separate else num_outputs * len(instruments)\n",
    "            module.output_conv = nn.Conv1d(num_channels[0], outputs, 1)\n",
    "\n",
    "            self.waveunets[instrument] = module\n",
    "\n",
    "        self.set_output_size(target_output_size)\n",
    "\n",
    "    def set_output_size(self, target_output_size):\n",
    "        self.target_output_size = target_output_size\n",
    "\n",
    "        self.input_size, self.output_size = self.check_padding(target_output_size)\n",
    "        print(\"Using valid convolutions with \" + str(self.input_size) + \" inputs and \" + str(self.output_size) + \" outputs\")\n",
    "\n",
    "        assert((self.input_size - self.output_size) % 2 == 0)\n",
    "        self.shapes = {\"output_start_frame\" : (self.input_size - self.output_size) // 2,\n",
    "                       \"output_end_frame\" : (self.input_size - self.output_size) // 2 + self.output_size,\n",
    "                       \"output_frames\" : self.output_size,\n",
    "                       \"input_frames\" : self.input_size}\n",
    "\n",
    "    def check_padding(self, target_output_size):\n",
    "        # Ensure number of outputs covers a whole number of cycles so each output in the cycle is weighted equally during training\n",
    "        bottleneck = 1\n",
    "\n",
    "        while True:\n",
    "            out = self.check_padding_for_bottleneck(bottleneck, target_output_size)\n",
    "            if out is not False:\n",
    "                return out\n",
    "            bottleneck += 1\n",
    "\n",
    "    def check_padding_for_bottleneck(self, bottleneck, target_output_size):\n",
    "        module = self.waveunets[[k for k in self.waveunets.keys()][0]]\n",
    "        try:\n",
    "            curr_size = bottleneck\n",
    "            for idx, block in enumerate(module.upsampling_blocks):\n",
    "                curr_size = block.get_output_size(curr_size)\n",
    "            output_size = curr_size\n",
    "\n",
    "            # Bottleneck-Conv\n",
    "            curr_size = bottleneck\n",
    "            for block in reversed(module.bottlenecks):\n",
    "                curr_size = block.get_input_size(curr_size)\n",
    "            for idx, block in enumerate(reversed(module.downsampling_blocks)):\n",
    "                curr_size = block.get_input_size(curr_size)\n",
    "\n",
    "            assert(output_size >= target_output_size)\n",
    "            return curr_size, output_size\n",
    "        except AssertionError as e:\n",
    "            return False\n",
    "\n",
    "    def forward_module(self, x, module):\n",
    "        '''\n",
    "        A forward pass through a single Wave-U-Net (multiple Wave-U-Nets might be used, one for each source)\n",
    "        :param x: Input mix\n",
    "        :param module: Network module to be used for prediction\n",
    "        :return: Source estimates\n",
    "        '''\n",
    "        shortcuts = []\n",
    "        out = x\n",
    "\n",
    "        # DOWNSAMPLING BLOCKS\n",
    "        for block in module.downsampling_blocks:\n",
    "            out, short = block(out)\n",
    "            shortcuts.append(short)\n",
    "\n",
    "        # BOTTLENECK CONVOLUTION\n",
    "        for conv in module.bottlenecks:\n",
    "            out = conv(out)\n",
    "\n",
    "        # UPSAMPLING BLOCKS\n",
    "        for idx, block in enumerate(module.upsampling_blocks):\n",
    "            out = block(out, shortcuts[-1 - idx])\n",
    "\n",
    "        # OUTPUT CONV\n",
    "        out = module.output_conv(out)\n",
    "        if not self.training:  # At test time clip predictions to valid amplitude range\n",
    "            out = out.clamp(min=-1.0, max=1.0)\n",
    "        return out\n",
    "\n",
    "    def forward(self, x, inst=None):\n",
    "        curr_input_size = x.shape[-1]\n",
    "        assert(curr_input_size == self.input_size) # User promises to feed the proper input himself, to get the pre-calculated (NOT the originally desired) output size\n",
    "\n",
    "        if self.separate:\n",
    "            return {inst : self.forward_module(x, self.waveunets[inst])}\n",
    "        else:\n",
    "            assert(len(self.waveunets) == 1)\n",
    "            out = self.forward_module(x, self.waveunets[\"ALL\"])\n",
    "\n",
    "            out_dict = {}\n",
    "            for idx, inst in enumerate(self.instruments):\n",
    "                out_dict[inst] = out[:, idx * self.num_outputs:(idx + 1) * self.num_outputs]\n",
    "            return out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "f49fd024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 5) Losses\n",
    "# ---------------------------\n",
    "\n",
    "def si_sdr(estimation, reference, eps=1e-8):\n",
    "    # estimation & reference: (B, C, T) or (B, 1, T)\n",
    "    # Returns SI-SDR in dB (per-example average over channels)\n",
    "    B, C, T = estimation.shape\n",
    "    est = estimation - estimation.mean(dim=-1, keepdim=True)\n",
    "    ref = reference - reference.mean(dim=-1, keepdim=True)\n",
    "\n",
    "    # projection of est on ref\n",
    "    ref_energy = torch.sum(ref * ref, dim=-1, keepdim=True) + eps\n",
    "    scalar = torch.sum(est * ref, dim=-1, keepdim=True) / ref_energy\n",
    "    s_target = scalar * ref\n",
    "    e_noise = est - s_target\n",
    "\n",
    "    target_energy = torch.sum(s_target ** 2, dim=-1)\n",
    "    noise_energy = torch.sum(e_noise ** 2, dim=-1) + eps\n",
    "    si_sdr_val = 10 * torch.log10((target_energy + eps) / noise_energy)\n",
    "    # average over channels then batch\n",
    "    return si_sdr_val.mean()\n",
    "\n",
    "\n",
    "def si_sdr_loss(estimation, reference):\n",
    "    # negative SI-SDR (assuming you already have a function si_sdr)\n",
    "    return -si_sdr(estimation, reference)\n",
    "\n",
    "def reconstruction_loss(target_est, residual_est, mix):\n",
    "    \"\"\"\n",
    "    Reconstruction term that aligns input (mix) with shorter model output.\n",
    "    \"\"\"\n",
    "   \n",
    "    return nn.functional.l1_loss(target_est + residual_est, mix)\n",
    "\n",
    "def combined_loss(target_est, residual_est, reference, mix):\n",
    "    \"\"\"\n",
    "    Final combined objective.\n",
    "    \"\"\"\n",
    "    return nn.MSELoss()(target_est + residual_est, mix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "359db5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 6) Metrics (MSE, MAE, SNR, SI-SDR)\n",
    "# ---------------------------\n",
    "def mse_metric(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    return float(np.mean((y_true - y_pred)**2))\n",
    "\n",
    "\n",
    "def mae_metric(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    return float(np.mean(np.abs(y_true - y_pred)))\n",
    "\n",
    "\n",
    "def snr_db_metric(y_true: np.ndarray, y_pred: np.ndarray, eps=1e-8) -> float:\n",
    "    # SNR = 20*log10(rms_true / rms_error)\n",
    "    rms_true = math.sqrt(np.mean(y_true**2) + eps)\n",
    "    rms_err = math.sqrt(np.mean((y_true - y_pred)**2) + eps)\n",
    "    return 20.0 * math.log10(rms_true / (rms_err + 1e-12))\n",
    "\n",
    "\n",
    "def si_sdr_metric(y_true: np.ndarray, y_pred: np.ndarray, eps=1e-8) -> float:\n",
    "    \"\"\"\n",
    "    Scale-Invariant SDR for single-channel signals\n",
    "    y_true, y_pred: 1D numpy arrays (same length)\n",
    "    \"\"\"\n",
    "    # remove mean\n",
    "    s = y_true.astype(np.float64) - np.mean(y_true)\n",
    "    s_hat = y_pred.astype(np.float64) - np.mean(y_pred)\n",
    "    # projection\n",
    "    s_target = (np.dot(s_hat, s) / (np.dot(s, s) + eps)) * s\n",
    "    e_noise = s_hat - s_target\n",
    "    num = np.sum(s_target**2)\n",
    "    den = np.sum(e_noise**2) + eps\n",
    "    return 10.0 * math.log10((num + eps) / den)\n",
    "\n",
    "\n",
    "def sdr_metric(y_true: np.ndarray, y_pred: np.ndarray, eps: float = 1e-8) -> float:\n",
    "    \"\"\"\n",
    "    Signal to Distortion Ratio (SDR)\n",
    "    \n",
    "    SDR = 10 * log10(||s_target||^2 / (||e_interf + e_noise + e_artif||^2))\n",
    "    \n",
    "    Args:\n",
    "        y_true: Reference signal (target)\n",
    "        y_pred: Estimated signal\n",
    "        eps: Small value to avoid division by zero\n",
    "    \n",
    "    Returns:\n",
    "        SDR in dB\n",
    "    \"\"\"\n",
    "    # Remove mean (optional but often done)\n",
    "    s = y_true.astype(np.float64) - np.mean(y_true)\n",
    "    s_hat = y_pred.astype(np.float64) - np.mean(y_pred)\n",
    "    \n",
    "    # Projection for target component\n",
    "    alpha = np.dot(s_hat, s) / (np.dot(s, s) + eps)\n",
    "    s_target = alpha * s\n",
    "    \n",
    "    # Error (distortion)\n",
    "    e_total = s_hat - s_target\n",
    "    \n",
    "    # Calculate energies\n",
    "    target_energy = np.sum(s_target**2)\n",
    "    distortion_energy = np.sum(e_total**2)\n",
    "    \n",
    "    return 10.0 * math.log10((target_energy + eps) / (distortion_energy + eps))\n",
    "\n",
    "\n",
    "def sir_metric(y_true: np.ndarray, y_pred: np.ndarray, eps: float = 1e-8) -> float:\n",
    "    \"\"\"\n",
    "    Signal to Interference Ratio (SIR)\n",
    "    \n",
    "    SIR = 10 * log10(||s_target||^2 / ||e_interf||^2)\n",
    "    \n",
    "    Args:\n",
    "        y_true: Reference signal (target)\n",
    "        y_pred: Estimated signal\n",
    "        eps: Small value to avoid division by zero\n",
    "    \n",
    "    Returns:\n",
    "        SIR in dB\n",
    "    \"\"\"\n",
    "    # Remove mean\n",
    "    s = y_true.astype(np.float64) - np.mean(y_true)\n",
    "    s_hat = y_pred.astype(np.float64) - np.mean(y_pred)\n",
    "    \n",
    "    # Projection for target component\n",
    "    alpha = np.dot(s_hat, s) / (np.dot(s, s) + eps)\n",
    "    s_target = alpha * s\n",
    "    \n",
    "    # For SIR, we need the interference component\n",
    "    # In single-channel case, interference is the part that correlates with other sources\n",
    "    # For simplicity in single-channel, we use the residual after removing target\n",
    "    e_interf = s_hat - s_target\n",
    "    \n",
    "    # Calculate energies\n",
    "    target_energy = np.sum(s_target**2)\n",
    "    interf_energy = np.sum(e_interf**2)\n",
    "    \n",
    "    return 10.0 * math.log10((target_energy + eps) / (interf_energy + eps))\n",
    "\n",
    "\n",
    "def sar_metric(y_true: np.ndarray, y_pred: np.ndarray, eps: float = 1e-8) -> float:\n",
    "    \"\"\"\n",
    "    Signal to Artifacts Ratio (SAR)\n",
    "    \n",
    "    SAR = 10 * log10(||s_target + e_interf||^2 / ||e_artif||^2)\n",
    "    \n",
    "    Args:\n",
    "        y_true: Reference signal (target)\n",
    "        y_pred: Estimated signal\n",
    "        eps: Small value to avoid division by zero\n",
    "    \n",
    "    Returns:\n",
    "        SAR in dB\n",
    "    \"\"\"\n",
    "    # Remove mean\n",
    "    s = y_true.astype(np.float64) - np.mean(y_true)\n",
    "    s_hat = y_pred.astype(np.float64) - np.mean(y_pred)\n",
    "    \n",
    "    # Projection for target component\n",
    "    alpha = np.dot(s_hat, s) / (np.dot(s, s) + eps)\n",
    "    s_target = alpha * s\n",
    "    \n",
    "    # In single-channel case, artifacts are typically the residual\n",
    "    # For SAR, we consider s_target + e_interf vs artifacts\n",
    "    # In single-channel context, artifacts ≈ e_noise\n",
    "    e_interf = s_hat - s_target\n",
    "    \n",
    "    # Signal + interference\n",
    "    signal_plus_interf = s_target + e_interf\n",
    "    \n",
    "    # For single-channel, artifacts are typically modeled as the non-linear distortions\n",
    "    # We approximate artifacts as the residual that cannot be explained by linear projection\n",
    "    e_artif = s_hat - signal_plus_interf  # This would be zero in linear model\n",
    "    \n",
    "    # More practical approach for single-channel SAR\n",
    "    signal_plus_interf_energy = np.sum(signal_plus_interf**2)\n",
    "    artifacts_energy = np.sum(e_artif**2) if np.sum(e_artif**2) > eps else eps\n",
    "    \n",
    "    return 10.0 * math.log10((signal_plus_interf_energy + eps) / (artifacts_energy + eps))\n",
    "\n",
    "\n",
    "def isr_metric(y_true: np.ndarray, y_pred: np.ndarray, eps: float = 1e-8) -> float:\n",
    "    \"\"\"\n",
    "    Image to Spatial distortion Ratio (ISR) - Also known as Source Image to Spatial distortion Ratio\n",
    "    \n",
    "    ISR = 10 * log10(||s_target||^2 / ||e_spat||^2)\n",
    "    \n",
    "    Note: In single-channel audio, ISR is less commonly used and may be approximated.\n",
    "    This implementation provides a reasonable approximation for single-channel case.\n",
    "    \n",
    "    Args:\n",
    "        y_true: Reference signal (target)\n",
    "        y_pred: Estimated signal\n",
    "        eps: Small value to avoid division by zero\n",
    "    \n",
    "    Returns:\n",
    "        ISR in dB\n",
    "    \"\"\"\n",
    "    # Remove mean\n",
    "    s = y_true.astype(np.float64) - np.mean(y_true)\n",
    "    s_hat = y_pred.astype(np.float64) - np.mean(y_pred)\n",
    "    \n",
    "    # Projection for target component\n",
    "    alpha = np.dot(s_hat, s) / (np.dot(s, s) + eps)\n",
    "    s_target = alpha * s\n",
    "    \n",
    "    # For ISR in single-channel, we approximate spatial distortion\n",
    "    # as the part that doesn't align with the target signal\n",
    "    e_spat = s_hat - s_target\n",
    "    \n",
    "    # Calculate energies\n",
    "    target_energy = np.sum(s_target**2)\n",
    "    spat_energy = np.sum(e_spat**2)\n",
    "    \n",
    "    return 10.0 * math.log10((target_energy + eps) / (spat_energy + eps))\n",
    "\n",
    "\n",
    "_METRIC_FUNCS = {\n",
    "    \"mse\": mse_metric,\n",
    "    \"mae\": mae_metric,\n",
    "    \"snr_db\": snr_db_metric,\n",
    "    \"si_sdr\": si_sdr_metric,\n",
    "    \"sdr\": sdr_metric,\n",
    "    \"sir\": sir_metric,\n",
    "    \"sar\": sar_metric,\n",
    "    \"isr\": isr_metric\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "bdabfe15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 6) train_epoch & validate_epoch\n",
    "# ---------------------------\n",
    "def train_epoch(model: nn.Module, loader: DataLoader, optimizer: torch.optim.Optimizer,\n",
    "                criterion: nn.Module, device: torch.device, clip_grad: Optional[float] = 5.0):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    n_samples = 0\n",
    "    pbar = tqdm(loader, desc=\"train\", leave=False)\n",
    "    for mix, target, residual in pbar:\n",
    "\n",
    "        input_size = model.shapes[\"input_frames\"]\n",
    "        output_size = model.target_output_size\n",
    "\n",
    "        # Original mix signal with sample size of output_size\n",
    "        orig_mix = mix.clone()\n",
    "\n",
    "        # Padding from left and right for mixed signal\n",
    "        sample_diff = input_size - mix.shape[-1]\n",
    "        if sample_diff > 0:\n",
    "            # pad left side\n",
    "            mix = nn.functional.pad(mix, (sample_diff // 2, 0))\n",
    "            # pad right side\n",
    "            mix = nn.functional.pad(mix, (0, sample_diff - sample_diff // 2))\n",
    "        else:\n",
    "            raise ValueError(f\"Expected a input_size > mix.shape[-1], but got {input_size} < {mix.shape[-1]}\")\n",
    "        \n",
    "        orig_mix = orig_mix.to(device)\n",
    "        mix = mix.to(device)\n",
    "        target = target.to(device)\n",
    "        residual = residual.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out_dict = model(mix)\n",
    "        target_est, residual_est = out_dict[\"target\"], out_dict[\"residual\"]\n",
    "\n",
    "        # Padding or cropping the target signal\n",
    "        sample_diff = output_size - target_est.shape[-1]\n",
    "        if sample_diff > 0:\n",
    "            # pad left side\n",
    "            target_est = nn.functional.pad(target_est, (sample_diff // 2, 0))\n",
    "            # pad right side\n",
    "            target_est = nn.functional.pad(target_est, (0, sample_diff - sample_diff // 2))\n",
    "        else:\n",
    "            # crop extra samples\n",
    "            target_est = target_est[..., :output_size]\n",
    "        \n",
    "        # Padding or cropping the residual signal\n",
    "        sample_diff = output_size - residual_est.shape[-1]\n",
    "        if sample_diff > 0:\n",
    "            # pad left side\n",
    "            residual_est = nn.functional.pad(residual_est, (sample_diff // 2, 0))\n",
    "            # pad right side\n",
    "            residual_est = nn.functional.pad(residual_est, (0, sample_diff - sample_diff // 2))\n",
    "        else:\n",
    "            # crop extra samples\n",
    "            residual_est = residual_est[..., :output_size]\n",
    "\n",
    "        loss = criterion(target_est, residual_est, target, orig_mix)\n",
    "        loss.backward()\n",
    "        if clip_grad:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip_grad)\n",
    "        optimizer.step()\n",
    "        batch_size = mix.shape[0]\n",
    "        running_loss += loss.item() * batch_size\n",
    "        n_samples += batch_size\n",
    "        pbar.set_postfix(loss=running_loss / n_samples)\n",
    "    return running_loss / max(1, n_samples)\n",
    "\n",
    "\n",
    "def validate_epoch(model: nn.Module, loader: DataLoader, criterion: nn.Module,\n",
    "                   device: torch.device, metric_names: List[str]):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    n_samples = 0\n",
    "    metric_sums = {m: 0.0 for m in metric_names}\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(loader, desc=\"val\", leave=False)\n",
    "        for mix, target, residual in pbar:\n",
    "            input_size = model.shapes[\"input_frames\"]\n",
    "            output_size = model.target_output_size\n",
    "\n",
    "            # Original mix signal with sample size of output_size\n",
    "            orig_mix = mix.clone()\n",
    "\n",
    "            # Padding from left and right for mixed signal\n",
    "            sample_diff = input_size - mix.shape[-1]\n",
    "            if sample_diff > 0:\n",
    "                # pad left side\n",
    "                mix = nn.functional.pad(mix, (sample_diff // 2, 0))\n",
    "                # pad right side\n",
    "                mix = nn.functional.pad(mix, (0, sample_diff - sample_diff // 2))\n",
    "            else:\n",
    "                raise ValueError(f\"Expected a input_size > mix.shape[-1], but got {input_size} < {mix.shape[-1]}\")\n",
    "            \n",
    "            orig_mix = orig_mix.to(device)\n",
    "            mix = mix.to(device)\n",
    "            target = target.to(device)\n",
    "            residual = residual.to(device)\n",
    "\n",
    "            out_dict = model(mix)\n",
    "            target_est, residual_est = out_dict[\"target\"], out_dict[\"residual\"]\n",
    "\n",
    "            # Padding or cropping the target signal\n",
    "            sample_diff = output_size - target_est.shape[-1]\n",
    "            if sample_diff > 0:\n",
    "                # pad left side\n",
    "                target_est = nn.functional.pad(target_est, (sample_diff // 2, 0))\n",
    "                # pad right side\n",
    "                target_est = nn.functional.pad(target_est, (0, sample_diff - sample_diff // 2))\n",
    "            else:\n",
    "                # crop extra samples\n",
    "                target_est = target_est[..., :output_size]\n",
    "            \n",
    "            # Padding or cropping the residual signal\n",
    "            sample_diff = output_size - residual_est.shape[-1]\n",
    "            if sample_diff > 0:\n",
    "                # pad left side\n",
    "                residual_est = nn.functional.pad(residual_est, (sample_diff // 2, 0))\n",
    "                # pad right side\n",
    "                residual_est = nn.functional.pad(residual_est, (0, sample_diff - sample_diff // 2))\n",
    "            else:\n",
    "                # crop extra samples\n",
    "                residual_est = residual_est[..., :output_size]\n",
    "\n",
    "            loss = criterion(target_est, residual_est, target, orig_mix)\n",
    "            batch_size = mix.shape[0]\n",
    "            running_loss += loss.item() * batch_size\n",
    "            n_samples += batch_size\n",
    "            # compute metrics sample-wise in numpy\n",
    "            est_np = target_est.detach().cpu().numpy()\n",
    "            clean_np = target.detach().cpu().numpy()\n",
    "            for b in range(batch_size):\n",
    "                y_true = clean_np[b, 0, :]\n",
    "                y_pred = est_np[b, 0, :]\n",
    "                for m in metric_names:\n",
    "                    metric_value = _METRIC_FUNCS[m](y_true, y_pred)\n",
    "                    metric_sums[m] += metric_value\n",
    "            pbar.set_postfix(loss=running_loss / n_samples)\n",
    "    avg_loss = running_loss / max(1, n_samples)\n",
    "    avg_metrics = {m: (metric_sums[m] / n_samples) for m in metric_names}\n",
    "    return avg_loss, avg_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "d3fcadab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 7) plotting helpers\n",
    "# ---------------------------\n",
    "def plot_history(history: dict, out_dir: str):\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    # loss curve\n",
    "    plt.figure()\n",
    "    plt.plot(history.get(\"train_loss\", []), label=\"train_loss\")\n",
    "    plt.plot(history.get(\"val_loss\", []), label=\"val_loss\")\n",
    "    plt.xlabel(\"epoch\"); plt.ylabel(\"loss\"); plt.legend(); plt.grid(True)\n",
    "    f1 = out_dir / \"loss_curve.png\"\n",
    "    plt.savefig(f1); plt.close()\n",
    "    # metrics\n",
    "    for m in history.get(\"metrics\", {}).keys():\n",
    "        plt.figure()\n",
    "        plt.plot(history[\"metrics\"][m], label=m)\n",
    "        plt.xlabel(\"epoch\"); plt.ylabel(m); plt.legend(); plt.grid(True)\n",
    "        plt.savefig(out_dir / f\"metric_{m}.png\")\n",
    "        plt.close()\n",
    "    return out_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "b7601df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 8) High-level train & evaluate pipeline\n",
    "# ---------------------------\n",
    "def train_and_evaluate(\n",
    "    processed_root: str,\n",
    "    dataset_name: str,\n",
    "    train_config_path: str,\n",
    "    metrics_config_path: str,\n",
    "    output_dir: str = \"model_output\",\n",
    "    mlflow_enabled: bool = False,\n",
    "    seed: int = 42\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    High-level entrypoint for training and evaluation. This is Airflow callable.\n",
    "    Returns a summary dict with final metrics and paths.\n",
    "    \"\"\"\n",
    "\n",
    "    # load configs\n",
    "    train_cfg, metrics_cfg = load_configs(train_config_path, metrics_config_path)\n",
    "    # set seeds\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # create train/test splits if needed\n",
    "    create_train_test_splits(processed_root, dataset_name, train_frac=float(train_cfg.get(\"train_frac\", 0.9)), seed=seed)\n",
    "\n",
    "    # dataset & dataloaders\n",
    "    train_dataset = WaveUNetDataset(processed_root, dataset_name, split=\"train\")\n",
    "    val_dataset = WaveUNetDataset(processed_root, dataset_name, split=\"test\")\n",
    "    train_loader = DataLoader(train_dataset, batch_size=train_cfg[\"batch_size\"], shuffle=True, num_workers=int(train_cfg.get(\"num_workers\", 4)), pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=train_cfg[\"batch_size\"], shuffle=False, num_workers=int(train_cfg.get(\"num_workers\", 4)), pin_memory=True)\n",
    "\n",
    "    # model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    kernel_size = int(train_cfg.get(\"kernel_size\", 15))\n",
    "\n",
    "    levels = int(train_cfg.get(\"levels\", 6))\n",
    "    features = int(train_cfg.get(\"features\", 32))\n",
    "    feature_growth = train_cfg.get(\"feature_growth\", \"add\")\n",
    "\n",
    "    depth = int(train_cfg.get(\"depth\", 1))\n",
    "    strides = int(train_cfg.get(\"stride\", 4))\n",
    "\n",
    "\n",
    "    num_features = [features*i for i in range(1, levels+1)] if feature_growth == \"add\" else \\\n",
    "        [features*2**i for i in range(0, levels)]\n",
    "\n",
    "    model = Waveunet(\n",
    "        num_inputs=1,\n",
    "        num_channels=num_features,\n",
    "        num_outputs=1,\n",
    "        instruments=['target', 'residual'],\n",
    "        kernel_size=kernel_size,\n",
    "        target_output_size=66150,\n",
    "        conv_type=\"gn\",\n",
    "        res=\"fixed\",\n",
    "        separate=False,\n",
    "        depth=depth,\n",
    "        strides=strides\n",
    "    )\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # criterion, optimizer, scheduler\n",
    "    criterion = combined_loss\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=float(train_cfg.get(\"lr\", 1e-4)))\n",
    "    scheduler = None\n",
    "    if train_cfg.get(\"lr_step\"):\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=train_cfg[\"lr_step\"], gamma=float(train_cfg.get(\"lr_gamma\", 0.5)))\n",
    "\n",
    "    # metrics and monitoring\n",
    "    metric_names = metrics_cfg.get(\"metrics\", [\"mse\"])\n",
    "    monitor = train_cfg.get(\"monitor_metric\", \"val_loss\")  # e.g. \"val_loss\" or \"si_sdr\"\n",
    "    monitor_mode = train_cfg.get(\"monitor_mode\", \"min\")   # \"min\" or \"max\"\n",
    "    best_score = math.inf if monitor_mode == \"min\" else -math.inf\n",
    "    best_ckpt_path = None\n",
    "\n",
    "    # output dirs\n",
    "    out_root = Path(output_dir)\n",
    "    ckpt_dir = out_root / \"checkpoints\"; ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "    plot_dir = out_root / \"plots\"; plot_dir.mkdir(parents=True, exist_ok=True)\n",
    "    history = {\"train_loss\": [], \"val_loss\": [], \"metrics\": {m: [] for m in metric_names}}\n",
    "\n",
    "    # MLflow start run\n",
    "    mlflow_run = None\n",
    "    if mlflow_enabled:\n",
    "        if not _mlflow_available:\n",
    "            logger.warning(\"MLflow requested but not available; continuing without MLflow.\")\n",
    "            mlflow_enabled = False\n",
    "        else:\n",
    "            mlflow.start_run()\n",
    "            mlflow_run = mlflow.active_run()\n",
    "            mlflow.log_params(train_cfg)\n",
    "            mlflow.log_params({\"metrics_cfg\": metrics_cfg})\n",
    "\n",
    "    num_epochs = int(train_cfg.get(\"epochs\", 50))\n",
    "    for epoch in range(num_epochs):\n",
    "        logger.info(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        t0 = time.time()\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, criterion, device, clip_grad=float(train_cfg.get(\"clip_grad\", 5.0)))\n",
    "        val_loss, val_metrics = validate_epoch(model, val_loader, criterion, device, metric_names)\n",
    "        logger.info(f\"Epoch {epoch+1} train_loss={train_loss:.6f} val_loss={val_loss:.6f} metrics={val_metrics} time={(time.time()-t0):.1f}s\")\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        for m in metric_names:\n",
    "            history[\"metrics\"][m].append(val_metrics.get(m, None))\n",
    "\n",
    "        # scheduler step\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        # monitoring and checkpointing\n",
    "        # If monitor_metric is \"val_loss\" use that; if it's in val_metrics use that.\n",
    "        if monitor == \"val_loss\":\n",
    "            current = val_loss\n",
    "        else:\n",
    "            current = val_metrics.get(monitor)\n",
    "            if current is None:\n",
    "                logger.warning(f\"Monitor metric {monitor} not found in val metrics; defaulting to val_loss.\")\n",
    "                current = val_loss\n",
    "\n",
    "        is_better = (current < best_score) if monitor_mode == \"min\" else (current > best_score)\n",
    "        if is_better:\n",
    "            best_score = current\n",
    "            best_ckpt_path = ckpt_dir / f\"best_{monitor}_{epoch+1}.pt\"\n",
    "            torch.save({\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"train_cfg\": train_cfg,\n",
    "                \"metrics_cfg\": metrics_cfg,\n",
    "                \"best_score\": best_score\n",
    "            }, str(best_ckpt_path))\n",
    "            logger.info(f\"Saved new best checkpoint: {best_ckpt_path}\")\n",
    "            if mlflow_enabled:\n",
    "                mlflow.log_metric(f\"best_{monitor}\", float(best_score), step=epoch)\n",
    "\n",
    "        # log epoch metrics to mlflow\n",
    "        if mlflow_enabled:\n",
    "            mlflow.log_metric(\"train_loss\", float(train_loss), step=epoch)\n",
    "            mlflow.log_metric(\"val_loss\", float(val_loss), step=epoch)\n",
    "            for m, v in val_metrics.items():\n",
    "                mlflow.log_metric(m, float(v), step=epoch)\n",
    "\n",
    "    # after training: load best model and evaluate on test set (here val set is test)\n",
    "    if best_ckpt_path is not None:\n",
    "        ckpt = torch.load(str(best_ckpt_path), map_location=device)\n",
    "        model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "        logger.info(f\"Loaded best model from {best_ckpt_path} for final evaluation.\")\n",
    "    else:\n",
    "        logger.warning(\"No checkpoint saved during training; using last model for evaluation.\")\n",
    "\n",
    "    # Final evaluation (on val/test loader)\n",
    "    final_loss, final_metrics = validate_epoch(model, val_loader, criterion, device, metric_names)\n",
    "    logger.info(f\"Final evaluation: loss={final_loss:.6f}, metrics={final_metrics}\")\n",
    "\n",
    "    # Save history & plots\n",
    "    hist_json = out_root / \"history.json\"\n",
    "    hist_json.write_text(json.dumps(history, indent=2))\n",
    "    plot_history(history, plot_dir)\n",
    "\n",
    "    # MLflow final logging & artifacts\n",
    "    if mlflow_enabled:\n",
    "        mlflow.log_metric(\"final_val_loss\", float(final_loss))\n",
    "        for m, v in final_metrics.items():\n",
    "            mlflow.log_metric(f\"final_{m}\", float(v))\n",
    "        # log artifacts\n",
    "        mlflow.log_artifacts(str(plot_dir), artifact_path=\"plots\")\n",
    "        mlflow.log_artifact(str(hist_json), artifact_path=\"history\")\n",
    "        if best_ckpt_path:\n",
    "            mlflow.log_artifact(str(best_ckpt_path), artifact_path=\"checkpoints\")\n",
    "        mlflow.end_run()\n",
    "\n",
    "    summary = {\n",
    "        \"best_checkpoint\": str(best_ckpt_path) if best_ckpt_path else None,\n",
    "        \"final_val_loss\": float(final_loss),\n",
    "        \"final_metrics\": final_metrics,\n",
    "        \"history_path\": str(hist_json),\n",
    "        \"plot_dir\": str(plot_dir)\n",
    "    }\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "a7c346a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:model_pipeline:Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/test subfolders already exist; skipping split creation.\n",
      "Using valid convolutions with 68237 inputs and 66165 outputs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:model_pipeline:Epoch 1 train_loss=0.080157 val_loss=0.002062 metrics={'mse': 0.00780771931167692, 'mae': 0.0666709529235959, 'snr_db': 1.7844986663677989, 'si_sdr': 10.788614429431686} time=4.5s\n",
      "INFO:model_pipeline:Saved new best checkpoint: model_output/checkpoints/best_snr_db_1.pt\n",
      "INFO:model_pipeline:Epoch 2/10\n",
      "INFO:model_pipeline:Epoch 2 train_loss=0.001079 val_loss=0.001101 metrics={'mse': 0.009318339102901519, 'mae': 0.07712731659412383, 'snr_db': 0.7483060713206292, 'si_sdr': 11.133012477296319} time=4.9s\n",
      "INFO:model_pipeline:Saved new best checkpoint: model_output/checkpoints/best_snr_db_2.pt\n",
      "INFO:model_pipeline:Epoch 3/10\n",
      "INFO:model_pipeline:Epoch 3 train_loss=0.000509 val_loss=0.000818 metrics={'mse': 0.009796246467158199, 'mae': 0.07916171066462993, 'snr_db': 0.5407234038804357, 'si_sdr': 10.50453961018179} time=4.7s\n",
      "INFO:model_pipeline:Saved new best checkpoint: model_output/checkpoints/best_snr_db_3.pt\n",
      "INFO:model_pipeline:Epoch 4/10\n",
      "INFO:model_pipeline:Epoch 4 train_loss=0.000342 val_loss=0.000608 metrics={'mse': 0.00940596596337855, 'mae': 0.07734616287052631, 'snr_db': 0.7488301593707264, 'si_sdr': 10.620995846665178} time=4.4s\n",
      "INFO:model_pipeline:Epoch 5/10\n",
      "INFO:model_pipeline:Epoch 5 train_loss=0.000256 val_loss=0.000524 metrics={'mse': 0.009499930660240352, 'mae': 0.07730746380984783, 'snr_db': 0.7339243054763127, 'si_sdr': 10.32297670319676} time=4.1s\n",
      "INFO:model_pipeline:Epoch 6/10\n",
      "INFO:model_pipeline:Epoch 6 train_loss=0.000208 val_loss=0.000409 metrics={'mse': 0.009310353035107254, 'mae': 0.0768266711384058, 'snr_db': 0.8179443275196124, 'si_sdr': 10.449232560482269} time=4.2s\n",
      "INFO:model_pipeline:Epoch 7/10\n",
      "INFO:model_pipeline:Epoch 7 train_loss=0.000170 val_loss=0.000366 metrics={'mse': 0.009319571149535476, 'mae': 0.07699736319482327, 'snr_db': 0.8141027310129492, 'si_sdr': 10.506047861318816} time=4.3s\n",
      "INFO:model_pipeline:Epoch 8/10\n",
      "INFO:model_pipeline:Epoch 8 train_loss=0.000149 val_loss=0.000330 metrics={'mse': 0.009331781882792711, 'mae': 0.07714912556111812, 'snr_db': 0.807566345343643, 'si_sdr': 10.418680377910784} time=4.7s\n",
      "INFO:model_pipeline:Epoch 9/10\n",
      "INFO:model_pipeline:Epoch 9 train_loss=0.000136 val_loss=0.000311 metrics={'mse': 0.009335960960015655, 'mae': 0.07717428021132947, 'snr_db': 0.8076110593245582, 'si_sdr': 10.406160710360775} time=4.5s\n",
      "INFO:model_pipeline:Epoch 10/10\n",
      "INFO:model_pipeline:Epoch 10 train_loss=0.000125 val_loss=0.000297 metrics={'mse': 0.009351393883116543, 'mae': 0.07723480649292469, 'snr_db': 0.8015476273357546, 'si_sdr': 10.37270390598621} time=4.3s\n",
      "INFO:model_pipeline:Loaded best model from model_output/checkpoints/best_snr_db_3.pt for final evaluation.\n",
      "INFO:model_pipeline:Final evaluation: loss=0.000818, metrics={'mse': 0.009796246467158199, 'mae': 0.07916171066462993, 'snr_db': 0.5407234038804357, 'si_sdr': 10.50453961018179}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'best_checkpoint': 'model_output/checkpoints/best_snr_db_3.pt',\n",
       " 'final_val_loss': 0.0008176200906746089,\n",
       " 'final_metrics': {'mse': 0.009796246467158199,\n",
       "  'mae': 0.07916171066462993,\n",
       "  'snr_db': 0.5407234038804357,\n",
       "  'si_sdr': 10.50453961018179},\n",
       " 'history_path': 'model_output/history.json',\n",
       " 'plot_dir': 'model_output/plots'}"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_and_evaluate(\n",
    "    processed_root=\"guitar_dataset/processed\",\n",
    "    dataset_name=\"dataset2-v2\",\n",
    "    train_config_path=\"train_config.yaml\",\n",
    "    metrics_config_path=\"metrics_config.yaml\",\n",
    "    mlflow_enabled=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cf4f07",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "68e978ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference_pipeline.py\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Optional, Dict\n",
    "\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "e5e3fc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------\n",
    "# Helpers: chunking & IO\n",
    "# ------------------------\n",
    "def split_audio_into_chunks(\n",
    "    audio: np.ndarray,\n",
    "    sample_rate: int,\n",
    "    chunk_duration: float\n",
    ") -> Tuple[List[np.ndarray], int]:\n",
    "    \"\"\"\n",
    "    Split 1D numpy audio into non-overlapping chunks of chunk_duration (seconds).\n",
    "    Pads the last chunk with zeros if needed.\n",
    "\n",
    "    Returns:\n",
    "      chunks: list of numpy arrays (each length == chunk_len)\n",
    "      orig_len: original audio length in samples (for trimming after reconstruction)\n",
    "    \"\"\"\n",
    "    if audio.ndim > 1:\n",
    "        audio = np.mean(audio, axis=1)  # to mono\n",
    "\n",
    "    orig_len = len(audio)\n",
    "    chunk_len = int(round(chunk_duration * sample_rate))\n",
    "    if chunk_len <= 0:\n",
    "        raise ValueError(\"chunk_duration too small for given sample_rate\")\n",
    "\n",
    "    n_chunks = (orig_len + chunk_len - 1) // chunk_len\n",
    "    chunks = []\n",
    "    for i in range(n_chunks):\n",
    "        s = i * chunk_len\n",
    "        e = s + chunk_len\n",
    "        if e <= orig_len:\n",
    "            chunks.append(audio[s:e].astype(np.float32))\n",
    "        else:\n",
    "            # pad right\n",
    "            pad = e - orig_len\n",
    "            chunk = np.pad(audio[s:orig_len], (0, pad), mode='constant').astype(np.float32)\n",
    "            chunks.append(chunk)\n",
    "    return chunks, orig_len\n",
    "\n",
    "\n",
    "def reconstruct_from_chunks(chunks_preds: List[np.ndarray], orig_len: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Concatenate list of 1D arrays (all same length) and trim to orig_len samples.\n",
    "    \"\"\"\n",
    "    if not chunks_preds:\n",
    "        return np.zeros(orig_len, dtype=np.float32)\n",
    "    out = np.concatenate(chunks_preds, axis=0)\n",
    "    if len(out) >= orig_len:\n",
    "        return out[:orig_len].astype(np.float32)\n",
    "    # if concatenated length is shorter (shouldn't happen), pad\n",
    "    pad = orig_len - len(out)\n",
    "    return np.pad(out, (0, pad), mode='constant').astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "9a51c591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------\n",
    "# Model loading\n",
    "# ------------------------\n",
    "def load_best_model(checkpoint_path: str, device: Optional[torch.device] = None):\n",
    "    \"\"\"\n",
    "    Loads a checkpoint saved by train_and_evaluate(...) and returns a model in eval mode.\n",
    "    The checkpoint is expected to contain 'model_state_dict' and optionally 'train_cfg' dict.\n",
    "    If train_cfg exists, it is used to re-create the WaveUNet1D with the same architecture.\n",
    "    \"\"\"\n",
    "    device = device or (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
    "    ckpt = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "    # try to infer model config\n",
    "    train_cfg = ckpt.get(\"train_cfg\") or ckpt.get(\"args\") or {}\n",
    "    kernel_size = int(train_cfg.get(\"kernel_size\", 15))\n",
    "\n",
    "    levels = int(train_cfg.get(\"levels\", 6))\n",
    "    features = int(train_cfg.get(\"features\", 32))\n",
    "    feature_growth = train_cfg.get(\"feature_growth\", \"add\")\n",
    "\n",
    "    depth = int(train_cfg.get(\"depth\", 1))\n",
    "    strides = int(train_cfg.get(\"stride\", 4))\n",
    "\n",
    "\n",
    "    num_features = [features*i for i in range(1, levels+1)] if feature_growth == \"add\" else \\\n",
    "        [features*2**i for i in range(0, levels)]\n",
    "\n",
    "    model = Waveunet(\n",
    "        num_inputs=1,\n",
    "        num_channels=num_features,\n",
    "        num_outputs=1,\n",
    "        instruments=['target', 'residual'],\n",
    "        kernel_size=kernel_size,\n",
    "        target_output_size=66150,\n",
    "        conv_type=\"gn\",\n",
    "        res=\"fixed\",\n",
    "        separate=False,\n",
    "        depth=depth,\n",
    "        strides=strides\n",
    "    )\n",
    "    \n",
    "    model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model, train_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "a5dd8749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------\n",
    "# Core inference for a single recording\n",
    "# ------------------------\n",
    "def infer_single_recording(\n",
    "    input_path: str,\n",
    "    model: torch.nn.Module,\n",
    "    sample_rate: int,\n",
    "    chunk_duration: float,\n",
    "    batch_size: int,\n",
    "    device: torch.device,\n",
    "    output_dir: str,\n",
    "    checkpoint_path: str,\n",
    "    dtype=np.float32\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Process one recording: split into chunks, run model in batches, reconstruct, save output and metadata.\n",
    "\n",
    "    Returns metadata dict with paths and basic stats.\n",
    "    \"\"\"\n",
    "    input_path = Path(input_path)\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # load and ensure mono\n",
    "    audio, sr = librosa.load(str(input_path), sr=sample_rate, mono=True)\n",
    "    chunks, orig_len = split_audio_into_chunks(audio, sample_rate, chunk_duration)\n",
    "\n",
    "    preds_target = []\n",
    "    preds_residual = []\n",
    "    model_device = device\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(chunks), batch_size):\n",
    "            batch_chunks = chunks[i:i + batch_size]\n",
    "            # build tensor shape (B,1,L)\n",
    "            batch_arr = np.stack(batch_chunks, axis=0)  # (B, L)\n",
    "            batch_tensor = torch.from_numpy(batch_arr).float().unsqueeze(1).to(model_device)  # (B,1,L)\n",
    "\n",
    "            input_size = model.shapes[\"input_frames\"]\n",
    "\n",
    "            # Padding from left and right for mixed signal\n",
    "            sample_diff = input_size - batch_tensor.shape[-1]\n",
    "            if sample_diff > 0:\n",
    "                # pad left side\n",
    "                batch_tensor = nn.functional.pad(batch_tensor, (sample_diff // 2, 0))\n",
    "                # pad right side\n",
    "                batch_tensor = nn.functional.pad(batch_tensor, (0, sample_diff - sample_diff // 2))\n",
    "            else:\n",
    "                raise ValueError(f\"Expected a input_size > mix.shape[-1], but got {input_size} < {mix.shape[-1]}\")\n",
    "\n",
    "            out_dict = model(batch_tensor)  # expected (B,1,L) thanks to model alignment\n",
    "            target_est, residual_est = out_dict[\"target\"], out_dict[\"residual\"]\n",
    "            target_tensor = target_est.detach().cpu().numpy()  # (B,1,L)\n",
    "            residual_tensor = residual_est.detach().cpu().numpy()\n",
    "            \n",
    "            out_arrs_target = [o[0].astype(dtype) for o in target_tensor]  # list of 1D arrays\n",
    "            out_arrs_residual = [o[0].astype(dtype) for o in residual_tensor]\n",
    "            preds_target.extend(out_arrs_target)\n",
    "            preds_residual.extend(out_arrs_residual)\n",
    "\n",
    "    # reconstruct\n",
    "    reconstructed_target = reconstruct_from_chunks(preds_target, orig_len)\n",
    "    reconstructed_residual = reconstruct_from_chunks(preds_residual, orig_len)\n",
    "\n",
    "    # save\n",
    "    out_basename = input_path.stem + \"_target_recon.wav\"\n",
    "    out_path = output_dir / out_basename\n",
    "    sf.write(str(out_path), reconstructed_target, sample_rate)\n",
    "\n",
    "    out_basename = input_path.stem + \"_resid_recon.wav\"\n",
    "    out_path = output_dir / out_basename\n",
    "    sf.write(str(out_path), reconstructed_residual, sample_rate)\n",
    "\n",
    "    # save metadata\n",
    "    meta = {\n",
    "        \"input_path\": str(input_path),\n",
    "        \"output_path\": str(out_path),\n",
    "        \"checkpoint\": str(checkpoint_path),\n",
    "        \"sample_rate\": int(sample_rate),\n",
    "        \"chunk_duration\": float(chunk_duration),\n",
    "        \"n_chunks\": int(len(chunks)),\n",
    "        \"orig_len_samples\": int(orig_len),\n",
    "        \"created_at\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime())\n",
    "    }\n",
    "    meta_path = output_dir / (input_path.stem + \"_meta.json\")\n",
    "    with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(meta, f, indent=2)\n",
    "\n",
    "    return meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "ec2691f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------\n",
    "# Top-level Airflow-friendly entrypoint\n",
    "# ------------------------\n",
    "def run_inference_pipeline(\n",
    "    input_root: str,\n",
    "    model_checkpoint: str,\n",
    "    output_root: str,\n",
    "    chunk_duration: float = 3.0,\n",
    "    sample_rate: int = 22050,\n",
    "    batch_size: int = 8,\n",
    "    device_str: Optional[str] = None,\n",
    "    glob_patterns: Optional[List[str]] = None,\n",
    "    max_files: Optional[int] = None\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Main function to be called by Airflow PythonOperator.\n",
    "\n",
    "    Parameters:\n",
    "      - input_root: folder with recordings (will search recursively)\n",
    "      - model_checkpoint: path to checkpoint (best model)\n",
    "      - output_root: where to save reconstructed recordings and metadata\n",
    "      - chunk_duration: seconds per chunk to pass to model\n",
    "      - sample_rate: sampling rate for loading/saving\n",
    "      - batch_size: inference batch size\n",
    "      - device_str: 'cuda' or 'cpu' (if None, automatically picked)\n",
    "      - glob_patterns: list of glob patterns to find files (default ['**/*.wav'])\n",
    "      - max_files: optional cap on number of recordings to process\n",
    "\n",
    "    Returns:\n",
    "      summary dict with list of processed files and metadata paths.\n",
    "    \"\"\"\n",
    "    device = torch.device(device_str if device_str is not None else (\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    model, train_cfg = load_best_model(model_checkpoint, device=device)\n",
    "\n",
    "    input_root = Path(input_root)\n",
    "    out_root = Path(output_root)\n",
    "    out_root.mkdir(parents=True, exist_ok=True)\n",
    "    glob_patterns = glob_patterns or [\"**/*.wav\"]\n",
    "\n",
    "    # collect files\n",
    "    files = []\n",
    "    for pat in glob_patterns:\n",
    "        files.extend(sorted(input_root.glob(pat)))\n",
    "    if not files:\n",
    "        raise RuntimeError(f\"No audio files found in {input_root} with patterns {glob_patterns}\")\n",
    "\n",
    "    if max_files is not None:\n",
    "        files = files[:max_files]\n",
    "\n",
    "    processed = []\n",
    "    pbar = tqdm(files, desc=\"Inference files\", dynamic_ncols=True)\n",
    "    for f in pbar:\n",
    "        try:\n",
    "            meta = infer_single_recording(\n",
    "                input_path=str(f),\n",
    "                model=model,\n",
    "                sample_rate=sample_rate,\n",
    "                chunk_duration=chunk_duration,\n",
    "                batch_size=batch_size,\n",
    "                device=device,\n",
    "                output_dir=str(out_root),\n",
    "                checkpoint_path=model_checkpoint\n",
    "            )\n",
    "            processed.append(meta)\n",
    "        except Exception as e:\n",
    "            # don't crash whole DAG on single file; instead record error\n",
    "            processed.append({\"input_path\": str(f), \"error\": str(e)})\n",
    "            # you may also choose to re-raise if you want failure semantics in Airflow\n",
    "            # raise\n",
    "\n",
    "    summary = {\n",
    "        \"model_checkpoint\": str(model_checkpoint),\n",
    "        \"n_files_requested\": len(files),\n",
    "        \"n_processed\": len(processed),\n",
    "        \"output_root\": str(out_root),\n",
    "        \"processed\": processed\n",
    "    }\n",
    "    # save summary\n",
    "    with open(out_root / \"inference_summary.json\", \"w\", encoding=\"utf-8\") as fh:\n",
    "        json.dump(summary, fh, indent=2)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "aadd72b9-46c0-4db6-8b6a-badc275962aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using valid convolutions with 99661 inputs and 66229 outputs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "301dcb67ac2e4e26bc0f4988a2f06068",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'model_checkpoint': 'model_output/checkpoints/best_snr_db_5.pt',\n",
       " 'n_files_requested': 1,\n",
       " 'n_processed': 1,\n",
       " 'output_root': 'guitar_dataset/processed/quality_test/recon',\n",
       " 'processed': [{'input_path': 'guitar_dataset/processed/quality_test/orig/quality_test_1.wav',\n",
       "   'output_path': 'guitar_dataset/processed/quality_test/recon/quality_test_1_resid_recon.wav',\n",
       "   'checkpoint': 'model_output/checkpoints/best_snr_db_5.pt',\n",
       "   'sample_rate': 22050,\n",
       "   'chunk_duration': 3.0,\n",
       "   'n_chunks': 6,\n",
       "   'orig_len_samples': 377732,\n",
       "   'created_at': '2025-10-06T14:06:37Z'}]}"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_inference_pipeline(\n",
    "    input_root=\"guitar_dataset/processed/quality_test/orig\",\n",
    "    model_checkpoint=\"model_output/checkpoints/best_snr_db_5.pt\",\n",
    "    output_root=\"guitar_dataset/processed/quality_test/recon\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4045c6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------\n",
    "# Example Airflow PythonOperator usage:\n",
    "# ------------------------\n",
    "# from airflow import DAG\n",
    "# from airflow.operators.python import PythonOperator\n",
    "# from datetime import datetime\n",
    "#\n",
    "# with DAG(\"waveunet_inference\", start_date=datetime(2025,10,4), schedule=None, catchup=False) as dag:\n",
    "#     infer_task = PythonOperator(\n",
    "#         task_id=\"run_inference\",\n",
    "#         python_callable=run_inference_pipeline,\n",
    "#         op_kwargs={\n",
    "#             \"input_root\": \"/mnt/data/to_process\",\n",
    "#             \"model_checkpoint\": \"/path/to/best_checkpoint.pt\",\n",
    "#             \"output_root\": \"/mnt/data/inference_out\",\n",
    "#             \"chunk_duration\": 3.0,\n",
    "#             \"sample_rate\": 22050,\n",
    "#             \"batch_size\": 8,\n",
    "#             \"device_str\": \"cuda\",\n",
    "#             \"glob_patterns\": [\"**/*.wav\"],\n",
    "#             \"max_files\": 200\n",
    "#         },\n",
    "#     )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
