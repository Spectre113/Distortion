{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e6a7fe0-1073-4415-ab46-21abc9e5f9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: soundfile in /opt/conda/lib/python3.10/site-packages (0.13.1)\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.10/site-packages (from soundfile) (1.16.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from soundfile) (1.26.2)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.0->soundfile) (2.21)\n"
     ]
    }
   ],
   "source": [
    "!pip install soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d951975-5172-4717-aada-3ed9dec5fcb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: librosa in /opt/conda/lib/python3.10/site-packages (0.11.0)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /opt/conda/lib/python3.10/site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /opt/conda/lib/python3.10/site-packages (from librosa) (0.58.1)\n",
      "Requirement already satisfied: numpy>=1.22.3 in /opt/conda/lib/python3.10/site-packages (from librosa) (1.26.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from librosa) (1.11.4)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from librosa) (1.3.2)\n",
      "Requirement already satisfied: joblib>=1.0 in /opt/conda/lib/python3.10/site-packages (from librosa) (1.3.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /opt/conda/lib/python3.10/site-packages (from librosa) (0.13.1)\n",
      "Requirement already satisfied: pooch>=1.1 in /opt/conda/lib/python3.10/site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /opt/conda/lib/python3.10/site-packages (from librosa) (1.0.0)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in /opt/conda/lib/python3.10/site-packages (from librosa) (4.8.0)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in /opt/conda/lib/python3.10/site-packages (from librosa) (0.3)\n",
      "Requirement already satisfied: msgpack>=1.0 in /opt/conda/lib/python3.10/site-packages (from librosa) (1.0.7)\n",
      "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba>=0.51.0->librosa) (0.41.1)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /opt/conda/lib/python3.10/site-packages (from pooch>=1.1->librosa) (4.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from pooch>=1.1->librosa) (23.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from pooch>=1.1->librosa) (2.31.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=1.1.0->librosa) (3.2.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.10/site-packages (from soundfile>=0.12.1->librosa) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2023.11.17)\n"
     ]
    }
   ],
   "source": [
    "!pip install librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261a439c-6adf-45be-9331-cd699c8449c0",
   "metadata": {},
   "source": [
    "## Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cce05a36-5b73-4cff-9858-da51fcee87e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "from typing import List, Tuple, Optional, Dict\n",
    "\n",
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from scipy import signal\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e42ce8b-f8c7-46db-a50f-afa1ac3226aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Noise generator (your function, slightly hardened)\n",
    "# -------------------------\n",
    "def create_custom_noise_profile(duration, sample_rate, overall_gain_db=-25):\n",
    "    \"\"\"Return a noise array length = duration*sample_rate (dtype=float32).\"\"\"\n",
    "    t = np.linspace(0, duration, int(duration * sample_rate), endpoint=False)\n",
    "    harmonic_noise = np.zeros_like(t)\n",
    "\n",
    "    # harmonic hum\n",
    "    fundamental_freqs = [440, 516, 645]\n",
    "    for fundamental in fundamental_freqs:\n",
    "        for harmonic in range(2, 7):\n",
    "            freq = fundamental * harmonic\n",
    "            detuned_freq = freq * (1 + np.random.uniform(-0.01, 0.01))\n",
    "            amplitude = 0.15 / harmonic\n",
    "            am_depth = 0.1\n",
    "            am_rate = 0.5\n",
    "            am_mod = 1 + am_depth * np.sin(2 * np.pi * am_rate * t)\n",
    "            harmonic_noise += amplitude * am_mod * np.sin(2 * np.pi * detuned_freq * t)\n",
    "\n",
    "    # resonant peaks (narrow band)\n",
    "    resonant_freqs = [3158, 3856, 5109]\n",
    "    resonant_amplitudes = [0.08, 0.08, 0.06]\n",
    "    resonant_noise = np.zeros_like(t)\n",
    "    nyquist = sample_rate / 2.0\n",
    "    for freq, amp in zip(resonant_freqs, resonant_amplitudes):\n",
    "        white = np.random.normal(0, 1, len(t))\n",
    "        low_cut = max(0.0001, (freq * 0.9) / nyquist)\n",
    "        high_cut = min(0.9999, (freq * 1.1) / nyquist)\n",
    "        try:\n",
    "            b, a = signal.butter(4, [low_cut, high_cut], btype='band')\n",
    "            narrow = signal.filtfilt(b, a, white)\n",
    "        except Exception:\n",
    "            narrow = white\n",
    "        resonant_noise += amp * narrow\n",
    "\n",
    "    # broadband hiss shaped by FIR\n",
    "    white_noise = np.random.normal(0, 1, len(t))\n",
    "    try:\n",
    "        from scipy.signal import firwin2\n",
    "        freq_points = [0, 1000, 1290, 1548, 1858, 2229, 2675, 4000, 6000, sample_rate/2]\n",
    "        gain_response = [10, 15, 15, 15, 15, 15, 15, 8, 5, 5]\n",
    "        norm = np.array(freq_points) / (sample_rate/2)\n",
    "        norm = np.clip(norm, 0.0, 1.0)\n",
    "        fir_coeffs = firwin2(1025, norm, gain_response)\n",
    "        shaped_hiss = signal.filtfilt(fir_coeffs, [1.0], white_noise)\n",
    "    except Exception:\n",
    "        shaped_hiss = white_noise\n",
    "\n",
    "    # combine -> apply notch -> normalize -> gain\n",
    "    combined = harmonic_noise + resonant_noise + shaped_hiss\n",
    "    try:\n",
    "        b_notch, a_notch = signal.iirnotch(3179.3, 4, sample_rate)\n",
    "        combined = signal.filtfilt(b_notch, a_notch, combined)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    maxabs = np.max(np.abs(combined)) + 1e-12\n",
    "    combined = combined / maxabs\n",
    "    gain_lin = 10 ** (overall_gain_db / 20.0)\n",
    "    return (combined * gain_lin).astype(np.float32)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# SAD: find non-silent intervals (librosa)\n",
    "# -------------------------\n",
    "def detect_activity_intervals(audio: np.ndarray, sr: int, top_db: float = 30.0, frame_length: int = 2048, hop_length: int = 512) -> List[Tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    Return list of (start_sample, end_sample) intervals containing activity.\n",
    "    Uses librosa.effects.split which is a simple SAD (energy thresholding).\n",
    "    top_db: threshold in dB below reference to consider silence (lower -> more aggressive keep)\n",
    "    \"\"\"\n",
    "    intervals = librosa.effects.split(y=audio, top_db=top_db, frame_length=frame_length, hop_length=hop_length)\n",
    "    return [(int(s), int(e)) for s, e in intervals]\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Helper: sample one fixed-length clip from non-silent intervals\n",
    "# -------------------------\n",
    "def sample_clip_from_intervals(audio: np.ndarray, sr: int, intervals: List[Tuple[int,int]], clip_duration: float, rng: Optional[random.Random] = None) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Choose a random interval that can contain a clip of clip_duration.\n",
    "    If no single interval is long enough, try to stitch or center-pad shorter audio to clip length.\n",
    "    Returns a numpy array of length = clip_duration * sr\n",
    "    \"\"\"\n",
    "    rng = rng or random\n",
    "    clip_len = int(round(clip_duration * sr))\n",
    "    # Filter intervals long enough\n",
    "    long_intervals = [iv for iv in intervals if (iv[1] - iv[0]) >= clip_len]\n",
    "    if long_intervals:\n",
    "        s, e = rng.choice(long_intervals)\n",
    "        start = rng.randint(s, e - clip_len)\n",
    "        clip = audio[start:start + clip_len]\n",
    "        return clip.astype(np.float32)\n",
    "    # otherwise try to sample from any interval, possibly concatenating up to clip_len by wrapping/padding:\n",
    "    if intervals:\n",
    "        # pick a random interval, extract it, then either pad or loop to reach clip_len\n",
    "        s, e = rng.choice(intervals)\n",
    "        seg = audio[s:e].astype(np.float32)\n",
    "        if len(seg) >= clip_len:\n",
    "            # deterministic crop\n",
    "            start = rng.randint(0, len(seg) - clip_len)\n",
    "            return seg[start:start+clip_len]\n",
    "        else:\n",
    "            # repeat or pad center\n",
    "            needed = clip_len - len(seg)\n",
    "            left = needed // 2\n",
    "            right = needed - left\n",
    "            return np.pad(seg, (left, right), mode='constant', constant_values=0.0)\n",
    "    # if no intervals (silent file) -> zero pad or use entire audio center\n",
    "    if len(audio) >= clip_len:\n",
    "        center = len(audio) // 2\n",
    "        start = max(0, center - clip_len // 2)\n",
    "        return audio[start:start + clip_len].astype(np.float32)\n",
    "    else:\n",
    "        return np.pad(audio.astype(np.float32), (0, clip_len - len(audio)), mode='constant')\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# RMS utilities\n",
    "# -------------------------\n",
    "def rms(x: np.ndarray, eps=1e-12) -> float:\n",
    "    return float(np.sqrt(np.mean(x.astype(np.float64)**2) + eps))\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Build one simulated mixture example (core of augmentation)\n",
    "# -------------------------\n",
    "def build_simulated_mixture(\n",
    "    stem_paths: List[Path],\n",
    "    sr: int,\n",
    "    clip_duration: float = 3.0,\n",
    "    min_stems: int = 1,\n",
    "    max_stems: int = 8,\n",
    "    energy_db_range: Tuple[float, float] = (-10.0, 10.0),\n",
    "    rng: Optional[random.Random] = None,\n",
    "    top_db: float = 30.0\n",
    ") -> Tuple[np.ndarray, List[Dict]]:\n",
    "    \"\"\"\n",
    "    Given a list of available stems (Paths), randomly select k stems (k in [min_stems, max_stems])\n",
    "    and produce a clean mixture (1D numpy array of length clip_duration*sr) and metadata list.\n",
    "    Metadata describes which stems, start samples, applied dB gains, original RMS.\n",
    "    \"\"\"\n",
    "    rng = rng or random\n",
    "    n_available = len(stem_paths)\n",
    "    if n_available == 0:\n",
    "        raise ValueError(\"No stems provided to build_simulated_mixture()\")\n",
    "\n",
    "    k = rng.randint(min_stems, min(max_stems, n_available))\n",
    "    selected = rng.sample(stem_paths, k)\n",
    "    clip_len = int(round(clip_duration * sr))\n",
    "\n",
    "    mixture = np.zeros(clip_len, dtype=np.float32)\n",
    "    metadata = []\n",
    "\n",
    "    for p in selected:\n",
    "        audio, file_sr = librosa.load(str(p), sr=None, mono=True)\n",
    "        # resample if needed\n",
    "        if file_sr != sr:\n",
    "            audio = librosa.resample(audio, orig_sr=file_sr, target_sr=sr)\n",
    "        intervals = detect_activity_intervals(audio, sr, top_db=top_db)\n",
    "        clip = sample_clip_from_intervals(audio, sr, intervals, clip_duration, rng=rng)\n",
    "        orig_rms = rms(clip)\n",
    "        db_change = rng.uniform(energy_db_range[0], energy_db_range[1])\n",
    "        gain_lin = 10 ** (db_change / 20.0)\n",
    "        scaled = (clip * gain_lin).astype(np.float32)\n",
    "        # sum to mixture\n",
    "        mixture = mixture + scaled\n",
    "        metadata.append({\n",
    "            \"stem_path\": str(p),\n",
    "            \"db_change\": float(db_change),\n",
    "            \"gain_lin\": float(gain_lin),\n",
    "            \"orig_rms\": float(orig_rms)\n",
    "        })\n",
    "\n",
    "    # After summing, avoid clipping: scale mixture by peak if needed, but preserve RMS relationships.\n",
    "    peak = float(np.max(np.abs(mixture)) + 1e-12)\n",
    "    if peak > 0.99:\n",
    "        mixture = (mixture / peak * 0.99).astype(np.float32)\n",
    "\n",
    "    return mixture, metadata\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Add synthetic noise to get input features + return metadata\n",
    "# -------------------------\n",
    "def add_noise_to_mixture(\n",
    "    clean_mixture: np.ndarray,\n",
    "    sr: int,\n",
    "    snr_db: float,\n",
    "    noise_func=create_custom_noise_profile,\n",
    "    overall_noise_gain_db: float = -25.0\n",
    ") -> Tuple[np.ndarray, Dict]:\n",
    "    \"\"\"\n",
    "    Create noise using noise_func, scale to target SNR with respect to clean_mixture RMS,\n",
    "    return noisy_mixture and noise metadata.\n",
    "    \"\"\"\n",
    "    duration = len(clean_mixture) / sr\n",
    "    noise = noise_func(duration, sr, overall_gain_db=overall_noise_gain_db)\n",
    "    if len(noise) > len(clean_mixture):\n",
    "        noise = noise[:len(clean_mixture)]\n",
    "    elif len(noise) < len(clean_mixture):\n",
    "        noise = np.pad(noise, (0, len(clean_mixture)-len(noise)))\n",
    "\n",
    "    rms_clean = rms(clean_mixture)\n",
    "    rms_noise = rms(noise)\n",
    "    target_lin = 10 ** (snr_db / 20.0)\n",
    "    required_noise_rms = (rms_clean / target_lin) if target_lin > 0 else rms_clean\n",
    "    noise_gain = (required_noise_rms / (rms_noise + 1e-12))\n",
    "    adjusted_noise = (noise * noise_gain).astype(np.float32)\n",
    "    noisy = clean_mixture + adjusted_noise\n",
    "\n",
    "    # prevent clipping\n",
    "    peak = float(np.max(np.abs(noisy)) + 1e-12)\n",
    "    if peak > 1.0:\n",
    "        noisy = (noisy / peak * 0.99).astype(np.float32)\n",
    "\n",
    "    meta = {\n",
    "        \"snr_db_target\": float(snr_db),\n",
    "        \"rms_clean\": float(rms_clean),\n",
    "        \"rms_noise_before_gain\": float(rms_noise),\n",
    "        \"noise_gain\": float(noise_gain),\n",
    "        \"overall_noise_profile_db\": float(overall_noise_gain_db)\n",
    "    }\n",
    "    return noisy, meta\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Pipeline orchestration (Airflow friendly)\n",
    "# -------------------------\n",
    "def run_augmentation_pipeline(\n",
    "    stems_root: str,\n",
    "    output_base: str = \"dataset\",\n",
    "    dataset_name: str = \"aug_mixtures_v1\",\n",
    "    sample_rate: int = 22050,\n",
    "    clip_duration: float = 3.0,\n",
    "    min_stems: int = 1,\n",
    "    max_stems: int = 8,\n",
    "    energy_db_range: Tuple[float,float] = (-10.0, 10.0),\n",
    "    snr_db_range: Tuple[float,float] = (5.0, 20.0),\n",
    "    max_files: Optional[int] = None,         # restrict number of stems considered (N); None -> all\n",
    "    n_examples: int = 1000,                  # number of augmented examples to synthesize\n",
    "    top_db: float = 30.0,\n",
    "    seed: Optional[int] = None\n",
    "):\n",
    "    \"\"\"\n",
    "    High-level pipeline:\n",
    "      - discover stems (N)\n",
    "      - limit to max_files if provided\n",
    "      - for i in [0..n_examples): create one mixture example:\n",
    "          - randomly choose between min_stems..max_stems stems\n",
    "          - sample clip_duration from each stem (using SAD)\n",
    "          - apply dB scaling per stem\n",
    "          - sum -> clean_mixture (target)\n",
    "          - sample snr in snr_db_range -> create noisy (feature)\n",
    "          - save noisy and clean wavs + metadata json\n",
    "    \"\"\"\n",
    "    rng = random.Random(seed)\n",
    "    stems_root = Path(stems_root)\n",
    "    stem_paths = sorted([p for p in stems_root.rglob(\"*\") if p.suffix.lower() in (\".wav\", \".flac\", \".mp3\")])\n",
    "    if not stem_paths:\n",
    "        raise RuntimeError(f\"No audio stems found in {stems_root}\")\n",
    "\n",
    "    if max_files is not None:\n",
    "        stem_paths = stem_paths[:max_files]\n",
    "\n",
    "    out_root = Path(output_base) / \"processed\" / dataset_name\n",
    "    clean_dir = out_root / \"clean\"\n",
    "    noisy_dir = out_root / \"noisy\"\n",
    "    meta_dir = out_root / \"meta\"\n",
    "    clean_dir.mkdir(parents=True, exist_ok=True)\n",
    "    noisy_dir.mkdir(parents=True, exist_ok=True)\n",
    "    meta_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(f\"Found {len(stem_paths)} stems. Will create {n_examples} examples using up to {max_stems} stems each.\")\n",
    "\n",
    "    for idx in tqdm(range(n_examples), desc=\"Synth examples\"):\n",
    "        # Build clean mixture\n",
    "        mixture, stems_meta = build_simulated_mixture(\n",
    "            stem_paths=stem_paths,\n",
    "            sr=sample_rate,\n",
    "            clip_duration=clip_duration,\n",
    "            min_stems=min_stems,\n",
    "            max_stems=max_stems,\n",
    "            energy_db_range=energy_db_range,\n",
    "            rng=rng,\n",
    "            top_db=top_db\n",
    "        )\n",
    "\n",
    "        # Choose an SNR for noise\n",
    "        snr = float(rng.uniform(snr_db_range[0], snr_db_range[1]))\n",
    "        noisy, noise_meta = add_noise_to_mixture(mixture, sr=sample_rate, snr_db=snr)\n",
    "\n",
    "        # Save files\n",
    "        basename = f\"example_{idx:06d}\"\n",
    "        clean_path = clean_dir / f\"{basename}_clean.wav\"\n",
    "        noisy_path = noisy_dir / f\"{basename}_noisy.wav\"\n",
    "        meta_path = meta_dir / f\"{basename}.json\"\n",
    "\n",
    "        sf.write(str(clean_path), mixture, sample_rate)\n",
    "        sf.write(str(noisy_path), noisy, sample_rate)\n",
    "\n",
    "        full_meta = {\n",
    "            \"example_id\": basename,\n",
    "            \"sample_rate\": int(sample_rate),\n",
    "            \"clip_duration\": float(clip_duration),\n",
    "            \"min_stems\": int(min_stems),\n",
    "            \"max_stems\": int(max_stems),\n",
    "            \"energy_db_range\": [float(energy_db_range[0]), float(energy_db_range[1])],\n",
    "            \"snr_db_range\": [float(snr_db_range[0]), float(snr_db_range[1])],\n",
    "            \"chosen_snr_db\": float(snr),\n",
    "            \"stems_meta\": stems_meta,\n",
    "            \"noise_meta\": noise_meta\n",
    "        }\n",
    "        with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(full_meta, f, indent=2)\n",
    "\n",
    "    print(f\"Saved {n_examples} examples to {out_root}\")\n",
    "    return str(out_root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fb107d1-1428-4f29-87fb-83d4376c631a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 261 stems. Will create 1000 examples using up to 8 stems each.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Synth examples:   0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Synth examples: 100%|██████████| 1000/1000 [08:22<00:00,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1000 examples to guitar_dataset\\processed\\dataset2-test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'guitar_dataset\\\\processed\\\\dataset2-test'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dir = \"IDMT-SMT-GUITAR_V2/dataset2/audio/\"\n",
    "\n",
    "run_augmentation_pipeline(\n",
    "    stems_root=input_dir,\n",
    "    output_base=\"guitar_dataset\",\n",
    "    dataset_name=\"dataset2-test\",\n",
    "    n_examples=1000,\n",
    "    seed=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f77fe1",
   "metadata": {},
   "source": [
    "## Model Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daecaff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "model_pipeline.py\n",
    "\n",
    "Airflow-friendly model engineering module:\n",
    "- load configs\n",
    "- dataset creation (clean/noisy pairs)\n",
    "- model build (Wave-U-Net 1D)\n",
    "- train_epoch / validate_epoch\n",
    "- checkpointing, plotting\n",
    "- optional MLflow logging\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import logging\n",
    "from typing import Tuple, Dict, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a6259584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional YAML support\n",
    "try:\n",
    "    import yaml\n",
    "except Exception:\n",
    "    yaml = None\n",
    "\n",
    "# Optional MLflow\n",
    "try:\n",
    "    import mlflow\n",
    "    _mlflow_available = True\n",
    "except Exception:\n",
    "    mlflow = None\n",
    "    _mlflow_available = False\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"model_pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a18e759c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 1) Config loading utility\n",
    "# ---------------------------\n",
    "def load_configs(train_config_path: str, metrics_config_path: str) -> Tuple[dict, dict]:\n",
    "    \"\"\"\n",
    "    Load training configuration and metrics/plot configuration from YAML or JSON.\n",
    "    Returns (train_cfg, metrics_cfg)\n",
    "    \"\"\"\n",
    "    def _load(path: str):\n",
    "        p = Path(path)\n",
    "        if not p.exists():\n",
    "            raise FileNotFoundError(f\"Config file not found: {path}\")\n",
    "        text = p.read_text(encoding=\"utf-8\")\n",
    "        if yaml and (p.suffix.lower() in (\".yml\", \".yaml\")):\n",
    "            return yaml.safe_load(text)\n",
    "        # try json\n",
    "        try:\n",
    "            return json.loads(text)\n",
    "        except Exception:\n",
    "            if yaml:\n",
    "                return yaml.safe_load(text)\n",
    "            raise\n",
    "\n",
    "    train_cfg = _load(train_config_path)\n",
    "    metrics_cfg = _load(metrics_config_path)\n",
    "    return train_cfg, metrics_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad4c2723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 2) Dataset for clean/noisy pairs\n",
    "# ---------------------------\n",
    "class CleanNoisyDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Loads pairs from processed dataset directory:\n",
    "      processed/{dataset_name}/clean/*.wav\n",
    "      processed/{dataset_name}/noisy/*.wav\n",
    "    It matches by basename prefix (e.g. example_000001_clean.wav vs example_000001_noisy.wav)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, processed_root: str, dataset_name: str, split: str = \"train\"):\n",
    "        \"\"\"\n",
    "        processed_root: base path (e.g., datasets/processed)\n",
    "        dataset_name: the dataset folder name\n",
    "        split: 'train' or 'test' - directory layout can contain subfolders 'train'/'test'\n",
    "               otherwise the module will perform an internal split (see helper function)\n",
    "        \"\"\"\n",
    "        base = Path(processed_root) / dataset_name\n",
    "        if not base.exists():\n",
    "            raise RuntimeError(f\"Processed dataset not found: {base}\")\n",
    "\n",
    "        # allow both: base/clean & base/noisy OR base/{split}/clean & base/{split}/noisy\n",
    "        possible_clean = base / \"clean\"\n",
    "        possible_noisy = base / \"noisy\"\n",
    "        alt_clean = base / split / \"clean\"\n",
    "        alt_noisy = base / split / \"noisy\"\n",
    "\n",
    "        if alt_clean.exists() and alt_noisy.exists():\n",
    "            self.clean_dir = alt_clean\n",
    "            self.noisy_dir = alt_noisy\n",
    "        elif possible_clean.exists() and possible_noisy.exists():\n",
    "            self.clean_dir = possible_clean\n",
    "            self.noisy_dir = possible_noisy\n",
    "        else:\n",
    "            raise RuntimeError(f\"Could not find clean/noisy directories in {base} or {base}/{split}\")\n",
    "\n",
    "        # build map of basenames\n",
    "        clean_files = sorted([p for p in self.clean_dir.glob(\"*.wav\")])\n",
    "        noisy_files = sorted([p for p in self.noisy_dir.glob(\"*.wav\")])\n",
    "\n",
    "        # map base prefix (without _clean/_noisy suffix) -> full path\n",
    "        def key_from_path(p: Path):\n",
    "            stem = p.stem\n",
    "            # remove common suffixes if present\n",
    "            for s in (\"_clean\", \"_noisy\"):\n",
    "                if stem.endswith(s):\n",
    "                    return stem[: -len(s)]\n",
    "            # if original naming is example_000001_clean -> returns example_000001\n",
    "            # else return whole stem\n",
    "            return stem\n",
    "\n",
    "        clean_map = {key_from_path(p): p for p in clean_files}\n",
    "        noisy_map = {key_from_path(p): p for p in noisy_files}\n",
    "        # intersect keys\n",
    "        keys = sorted(list(set(clean_map.keys()) & set(noisy_map.keys())))\n",
    "        if not keys:\n",
    "            raise RuntimeError(f\"No matching clean/noisy pairs found in {self.clean_dir} and {self.noisy_dir}\")\n",
    "\n",
    "        self.pairs = [(clean_map[k], noisy_map[k]) for k in keys]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        clean_p, noisy_p = self.pairs[idx]\n",
    "        # load\n",
    "        clean, sr1 = sf.read(str(clean_p))\n",
    "        noisy, sr2 = sf.read(str(noisy_p))\n",
    "        # ensure mono and same sr\n",
    "        if clean.ndim > 1:\n",
    "            clean = np.mean(clean, axis=1)\n",
    "        if noisy.ndim > 1:\n",
    "            noisy = np.mean(noisy, axis=1)\n",
    "        if sr1 != sr2:\n",
    "            raise RuntimeError(f\"Sample rates mismatch between {clean_p} and {noisy_p}\")\n",
    "        # convert to float32 and tensor shape (1, L)\n",
    "        clean = torch.from_numpy(np.asarray(clean, dtype=np.float32)).unsqueeze(0)\n",
    "        noisy = torch.from_numpy(np.asarray(noisy, dtype=np.float32)).unsqueeze(0)\n",
    "        return noisy, clean  # feature, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "759c79b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 3) Splitting helper\n",
    "# ---------------------------\n",
    "def create_train_test_splits(processed_root: str, dataset_name: str, train_frac: float = 0.9, seed: int = 42, out_dir: Optional[str] = None):\n",
    "    \"\"\"\n",
    "    If the processed dataset does not contain explicit train/test folders, create splits by copying files.\n",
    "    This function will create processed/{dataset_name}/train/{clean,noisy} and /test/{clean,noisy}\n",
    "    and copy files accordingly. If those folders already exist, it does nothing.\n",
    "    Returns (train_dataset_dir, test_dataset_dir) as paths to the processed root (which can be used by CleanNoisyDataset with split='train'/'test').\n",
    "    \"\"\"\n",
    "    base = Path(processed_root) / dataset_name\n",
    "    train_clean = base / \"train\" / \"clean\"\n",
    "    train_noisy = base / \"train\" / \"noisy\"\n",
    "    test_clean = base / \"test\" / \"clean\"\n",
    "    test_noisy = base / \"test\" / \"noisy\"\n",
    "\n",
    "    # if already split, return\n",
    "    if train_clean.exists() and test_clean.exists():\n",
    "        logger.info(\"Train/test subfolders already exist; skipping split creation.\")\n",
    "        return str(base), str(base)\n",
    "\n",
    "    # otherwise create split\n",
    "    all_clean = sorted([p for p in (base / \"clean\").glob(\"*.wav\")])\n",
    "    all_noisy = sorted([p for p in (base / \"noisy\").glob(\"*.wav\")])\n",
    "    if not all_clean or not all_noisy:\n",
    "        raise RuntimeError(f\"No clean/noisy files found under {base}\")\n",
    "\n",
    "    # match keys (same logic as dataset)\n",
    "    def key(p: Path):\n",
    "        s = p.stem\n",
    "        for suf in (\"_clean\", \"_noisy\"):\n",
    "            if s.endswith(suf):\n",
    "                return s[:-len(suf)]\n",
    "        return s\n",
    "\n",
    "    clean_map = {key(p): p for p in all_clean}\n",
    "    noisy_map = {key(p): p for p in all_noisy}\n",
    "    keys = sorted(list(set(clean_map.keys()) & set(noisy_map.keys())))\n",
    "    random.Random(seed).shuffle(keys)\n",
    "    n_train = int(math.floor(len(keys) * train_frac))\n",
    "    train_keys = keys[:n_train]\n",
    "    test_keys = keys[n_train:]\n",
    "\n",
    "    # create directories\n",
    "    train_clean.mkdir(parents=True, exist_ok=True)\n",
    "    train_noisy.mkdir(parents=True, exist_ok=True)\n",
    "    test_clean.mkdir(parents=True, exist_ok=True)\n",
    "    test_noisy.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    from shutil import copy2\n",
    "    for k in train_keys:\n",
    "        copy2(clean_map[k], train_clean / clean_map[k].name)\n",
    "        copy2(noisy_map[k], train_noisy / noisy_map[k].name)\n",
    "    for k in test_keys:\n",
    "        copy2(clean_map[k], test_clean / clean_map[k].name)\n",
    "        copy2(noisy_map[k], test_noisy / noisy_map[k].name)\n",
    "\n",
    "    logger.info(f\"Created train/test split: {len(train_keys)} train, {len(test_keys)} test\")\n",
    "    return str(base), str(base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03aac85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 4) Wave U-Net model (same as earlier, slightly improved)\n",
    "# ---------------------------\n",
    "class ConvBlock1D(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, kernel_size=15, stride=1, padding=None, activation=nn.LeakyReLU(0.2)):\n",
    "        super().__init__()\n",
    "        if padding is None:\n",
    "            padding = (kernel_size - 1) // 2\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(in_ch, out_ch, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            nn.BatchNorm1d(out_ch),\n",
    "            activation\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "class WaveUNet1D(nn.Module):\n",
    "    def __init__(self, input_channels=1, output_channels=1, base_filters=24, depth=5, kernel_size=15):\n",
    "        super().__init__()\n",
    "        self.depth = depth\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.ups = nn.ModuleList()\n",
    "        in_ch = input_channels\n",
    "        for d in range(depth):\n",
    "            out_ch = base_filters * (2 ** d)\n",
    "            self.downs.append(\n",
    "                nn.Sequential(\n",
    "                    ConvBlock1D(in_ch, out_ch, kernel_size=kernel_size, stride=1),\n",
    "                    ConvBlock1D(out_ch, out_ch, kernel_size=kernel_size, stride=2)  # downsample\n",
    "                )\n",
    "            )\n",
    "            in_ch = out_ch\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            ConvBlock1D(in_ch, in_ch * 2, kernel_size=kernel_size, stride=1),\n",
    "            ConvBlock1D(in_ch * 2, in_ch, kernel_size=kernel_size, stride=1)\n",
    "        )\n",
    "        # for d in reversed(range(depth)):\n",
    "        #     in_ch = base_filters * (2 ** d)\n",
    "        #     # up conv expects concatenated channels (skip connection)\n",
    "        #     self.ups.append(\n",
    "        #         nn.Sequential(\n",
    "        #             ConvBlock1D(in_ch * 2, in_ch, kernel_size=kernel_size, stride=1),\n",
    "        #             ConvBlock1D(in_ch, in_ch, kernel_size=kernel_size, stride=1)\n",
    "        #         )\n",
    "        #     )\n",
    "\n",
    "        # At this point `in_ch` is the channel count after the encoder (and matches bottleneck output)\n",
    "        cur_channels = in_ch  # channels flowing into the decoder at first stage (deepest)\n",
    "        self.ups = nn.ModuleList()\n",
    "        for d in reversed(range(depth)):\n",
    "            skip_ch = base_filters * (2 ** d)           # channels from this encoder skip\n",
    "            concat_ch = cur_channels + skip_ch          # actual channels after torch.cat([cur, skip], dim=1)\n",
    "            out_ch = skip_ch                            # we reduce to skip_ch (common UNet pattern)\n",
    "\n",
    "            self.ups.append(\n",
    "                nn.Sequential(\n",
    "                    ConvBlock1D(concat_ch, out_ch, kernel_size=kernel_size, stride=1),\n",
    "                    ConvBlock1D(out_ch, out_ch, kernel_size=kernel_size, stride=1)\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # update cur_channels for next (upper) decoder stage\n",
    "            cur_channels = out_ch\n",
    "        self.final_conv = nn.Conv1d(base_filters, output_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skips = []\n",
    "        cur = x\n",
    "        for block in self.downs:\n",
    "            cur = block(cur)\n",
    "            skips.append(cur)\n",
    "        cur = self.bottleneck(cur)\n",
    "        for up_block, skip in zip(self.ups, reversed(skips)):\n",
    "            cur = nn.functional.interpolate(cur, size=skip.shape[-1], mode='linear', align_corners=False)\n",
    "            # equalize length\n",
    "            if cur.shape[-1] != skip.shape[-1]:\n",
    "                diff = skip.shape[-1] - cur.shape[-1]\n",
    "                if diff > 0:\n",
    "                    cur = nn.functional.pad(cur, (0, diff))\n",
    "                else:\n",
    "                    cur = cur[..., :skip.shape[-1]]\n",
    "            cur = torch.cat([cur, skip], dim=1)\n",
    "            cur = up_block(cur)\n",
    "        if cur.shape[1] != self.final_conv.in_channels:\n",
    "            adjust = nn.Conv1d(cur.shape[1], self.final_conv.in_channels, kernel_size=1).to(cur.device)\n",
    "            cur = adjust(cur)\n",
    "        out = self.final_conv(cur)\n",
    "        out = torch.tanh(out)\n",
    "\n",
    "        # Ensure output length matches input length\n",
    "        if out.shape[-1] != x.shape[-1]:\n",
    "            diff = x.shape[-1] - out.shape[-1]\n",
    "            if diff > 0:\n",
    "                # pad right side\n",
    "                out = nn.functional.pad(out, (0, diff))\n",
    "            else:\n",
    "                # crop extra samples\n",
    "                out = out[..., :x.shape[-1]]\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359db5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 5) Metrics (MSE, MAE, SNR, SI-SDR)\n",
    "# ---------------------------\n",
    "def mse_metric(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    return float(np.mean((y_true - y_pred)**2))\n",
    "\n",
    "\n",
    "def mae_metric(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    return float(np.mean(np.abs(y_true - y_pred)))\n",
    "\n",
    "\n",
    "def snr_db_metric(y_true: np.ndarray, y_pred: np.ndarray, eps=1e-8) -> float:\n",
    "    # SNR = 20*log10(rms_true / rms_error)\n",
    "    rms_true = math.sqrt(np.mean(y_true**2) + eps)\n",
    "    rms_err = math.sqrt(np.mean((y_true - y_pred)**2) + eps)\n",
    "    return 20.0 * math.log10(rms_true / (rms_err + 1e-12))\n",
    "\n",
    "\n",
    "def si_sdr_metric(y_true: np.ndarray, y_pred: np.ndarray, eps=1e-8) -> float:\n",
    "    \"\"\"\n",
    "    Scale-Invariant SDR for single-channel signals\n",
    "    y_true, y_pred: 1D numpy arrays (same length)\n",
    "    \"\"\"\n",
    "    # remove mean\n",
    "    s = y_true.astype(np.float64) - np.mean(y_true)\n",
    "    s_hat = y_pred.astype(np.float64) - np.mean(y_pred)\n",
    "    # projection\n",
    "    s_target = (np.dot(s_hat, s) / (np.dot(s, s) + eps)) * s\n",
    "    e_noise = s_hat - s_target\n",
    "    num = np.sum(s_target**2)\n",
    "    den = np.sum(e_noise**2) + eps\n",
    "    return 10.0 * math.log10((num + eps) / den)\n",
    "\n",
    "\n",
    "def sdr_metric(y_true: np.ndarray, y_pred: np.ndarray, eps: float = 1e-8) -> float:\n",
    "    \"\"\"\n",
    "    Signal to Distortion Ratio (SDR)\n",
    "    \n",
    "    SDR = 10 * log10(||s_target||^2 / (||e_interf + e_noise + e_artif||^2))\n",
    "    \n",
    "    Args:\n",
    "        y_true: Reference signal (target)\n",
    "        y_pred: Estimated signal\n",
    "        eps: Small value to avoid division by zero\n",
    "    \n",
    "    Returns:\n",
    "        SDR in dB\n",
    "    \"\"\"\n",
    "    # Remove mean (optional but often done)\n",
    "    s = y_true.astype(np.float64) - np.mean(y_true)\n",
    "    s_hat = y_pred.astype(np.float64) - np.mean(y_pred)\n",
    "    \n",
    "    # Projection for target component\n",
    "    alpha = np.dot(s_hat, s) / (np.dot(s, s) + eps)\n",
    "    s_target = alpha * s\n",
    "    \n",
    "    # Error (distortion)\n",
    "    e_total = s_hat - s_target\n",
    "    \n",
    "    # Calculate energies\n",
    "    target_energy = np.sum(s_target**2)\n",
    "    distortion_energy = np.sum(e_total**2)\n",
    "    \n",
    "    return 10.0 * math.log10((target_energy + eps) / (distortion_energy + eps))\n",
    "\n",
    "\n",
    "def sir_metric(y_true: np.ndarray, y_pred: np.ndarray, eps: float = 1e-8) -> float:\n",
    "    \"\"\"\n",
    "    Signal to Interference Ratio (SIR)\n",
    "    \n",
    "    SIR = 10 * log10(||s_target||^2 / ||e_interf||^2)\n",
    "    \n",
    "    Args:\n",
    "        y_true: Reference signal (target)\n",
    "        y_pred: Estimated signal\n",
    "        eps: Small value to avoid division by zero\n",
    "    \n",
    "    Returns:\n",
    "        SIR in dB\n",
    "    \"\"\"\n",
    "    # Remove mean\n",
    "    s = y_true.astype(np.float64) - np.mean(y_true)\n",
    "    s_hat = y_pred.astype(np.float64) - np.mean(y_pred)\n",
    "    \n",
    "    # Projection for target component\n",
    "    alpha = np.dot(s_hat, s) / (np.dot(s, s) + eps)\n",
    "    s_target = alpha * s\n",
    "    \n",
    "    # For SIR, we need the interference component\n",
    "    # In single-channel case, interference is the part that correlates with other sources\n",
    "    # For simplicity in single-channel, we use the residual after removing target\n",
    "    e_interf = s_hat - s_target\n",
    "    \n",
    "    # Calculate energies\n",
    "    target_energy = np.sum(s_target**2)\n",
    "    interf_energy = np.sum(e_interf**2)\n",
    "    \n",
    "    return 10.0 * math.log10((target_energy + eps) / (interf_energy + eps))\n",
    "\n",
    "\n",
    "def sar_metric(y_true: np.ndarray, y_pred: np.ndarray, eps: float = 1e-8) -> float:\n",
    "    \"\"\"\n",
    "    Signal to Artifacts Ratio (SAR)\n",
    "    \n",
    "    SAR = 10 * log10(||s_target + e_interf||^2 / ||e_artif||^2)\n",
    "    \n",
    "    Args:\n",
    "        y_true: Reference signal (target)\n",
    "        y_pred: Estimated signal\n",
    "        eps: Small value to avoid division by zero\n",
    "    \n",
    "    Returns:\n",
    "        SAR in dB\n",
    "    \"\"\"\n",
    "    # Remove mean\n",
    "    s = y_true.astype(np.float64) - np.mean(y_true)\n",
    "    s_hat = y_pred.astype(np.float64) - np.mean(y_pred)\n",
    "    \n",
    "    # Projection for target component\n",
    "    alpha = np.dot(s_hat, s) / (np.dot(s, s) + eps)\n",
    "    s_target = alpha * s\n",
    "    \n",
    "    # In single-channel case, artifacts are typically the residual\n",
    "    # For SAR, we consider s_target + e_interf vs artifacts\n",
    "    # In single-channel context, artifacts ≈ e_noise\n",
    "    e_interf = s_hat - s_target\n",
    "    \n",
    "    # Signal + interference\n",
    "    signal_plus_interf = s_target + e_interf\n",
    "    \n",
    "    # For single-channel, artifacts are typically modeled as the non-linear distortions\n",
    "    # We approximate artifacts as the residual that cannot be explained by linear projection\n",
    "    e_artif = s_hat - signal_plus_interf  # This would be zero in linear model\n",
    "    \n",
    "    # More practical approach for single-channel SAR\n",
    "    signal_plus_interf_energy = np.sum(signal_plus_interf**2)\n",
    "    artifacts_energy = np.sum(e_artif**2) if np.sum(e_artif**2) > eps else eps\n",
    "    \n",
    "    return 10.0 * math.log10((signal_plus_interf_energy + eps) / (artifacts_energy + eps))\n",
    "\n",
    "\n",
    "def isr_metric(y_true: np.ndarray, y_pred: np.ndarray, eps: float = 1e-8) -> float:\n",
    "    \"\"\"\n",
    "    Image to Spatial distortion Ratio (ISR) - Also known as Source Image to Spatial distortion Ratio\n",
    "    \n",
    "    ISR = 10 * log10(||s_target||^2 / ||e_spat||^2)\n",
    "    \n",
    "    Note: In single-channel audio, ISR is less commonly used and may be approximated.\n",
    "    This implementation provides a reasonable approximation for single-channel case.\n",
    "    \n",
    "    Args:\n",
    "        y_true: Reference signal (target)\n",
    "        y_pred: Estimated signal\n",
    "        eps: Small value to avoid division by zero\n",
    "    \n",
    "    Returns:\n",
    "        ISR in dB\n",
    "    \"\"\"\n",
    "    # Remove mean\n",
    "    s = y_true.astype(np.float64) - np.mean(y_true)\n",
    "    s_hat = y_pred.astype(np.float64) - np.mean(y_pred)\n",
    "    \n",
    "    # Projection for target component\n",
    "    alpha = np.dot(s_hat, s) / (np.dot(s, s) + eps)\n",
    "    s_target = alpha * s\n",
    "    \n",
    "    # For ISR in single-channel, we approximate spatial distortion\n",
    "    # as the part that doesn't align with the target signal\n",
    "    e_spat = s_hat - s_target\n",
    "    \n",
    "    # Calculate energies\n",
    "    target_energy = np.sum(s_target**2)\n",
    "    spat_energy = np.sum(e_spat**2)\n",
    "    \n",
    "    return 10.0 * math.log10((target_energy + eps) / (spat_energy + eps))\n",
    "\n",
    "\n",
    "_METRIC_FUNCS = {\n",
    "    \"mse\": mse_metric,\n",
    "    \"mae\": mae_metric,\n",
    "    \"snr_db\": snr_db_metric,\n",
    "    \"si_sdr\": si_sdr_metric,\n",
    "    \"sdr\": sdr_metric,\n",
    "    \"sir\": sir_metric,\n",
    "    \"sar\": sar_metric,\n",
    "    \"isr\": isr_metric\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bdabfe15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 6) train_epoch & validate_epoch\n",
    "# ---------------------------\n",
    "def train_epoch(model: nn.Module, loader: DataLoader, optimizer: torch.optim.Optimizer,\n",
    "                criterion: nn.Module, device: torch.device, clip_grad: Optional[float] = 5.0):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    n_samples = 0\n",
    "    pbar = tqdm(loader, desc=\"train\", leave=False)\n",
    "    for noisy, clean in pbar:\n",
    "        noisy = noisy.to(device)\n",
    "        clean = clean.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        est = model(noisy)\n",
    "        loss = criterion(est, clean)\n",
    "        loss.backward()\n",
    "        if clip_grad:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip_grad)\n",
    "        optimizer.step()\n",
    "        batch_size = noisy.shape[0]\n",
    "        running_loss += loss.item() * batch_size\n",
    "        n_samples += batch_size\n",
    "        pbar.set_postfix(loss=running_loss / n_samples)\n",
    "    return running_loss / max(1, n_samples)\n",
    "\n",
    "\n",
    "def validate_epoch(model: nn.Module, loader: DataLoader, criterion: nn.Module,\n",
    "                   device: torch.device, metric_names: List[str]):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    n_samples = 0\n",
    "    metric_sums = {m: 0.0 for m in metric_names}\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(loader, desc=\"val\", leave=False)\n",
    "        for noisy, clean in pbar:\n",
    "            noisy = noisy.to(device)\n",
    "            clean = clean.to(device)\n",
    "            est = model(noisy)\n",
    "            loss = criterion(est, clean)\n",
    "            batch_size = noisy.shape[0]\n",
    "            running_loss += loss.item() * batch_size\n",
    "            n_samples += batch_size\n",
    "            # compute metrics sample-wise in numpy\n",
    "            est_np = est.detach().cpu().numpy()\n",
    "            clean_np = clean.detach().cpu().numpy()\n",
    "            for b in range(batch_size):\n",
    "                y_true = clean_np[b, 0, :]\n",
    "                y_pred = est_np[b, 0, :]\n",
    "                for m in metric_names:\n",
    "                    metric_value = _METRIC_FUNCS[m](y_true, y_pred)\n",
    "                    metric_sums[m] += metric_value\n",
    "            pbar.set_postfix(loss=running_loss / n_samples)\n",
    "    avg_loss = running_loss / max(1, n_samples)\n",
    "    avg_metrics = {m: (metric_sums[m] / n_samples) for m in metric_names}\n",
    "    return avg_loss, avg_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d3fcadab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 7) plotting helpers\n",
    "# ---------------------------\n",
    "def plot_history(history: dict, out_dir: str):\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    # loss curve\n",
    "    plt.figure()\n",
    "    plt.plot(history.get(\"train_loss\", []), label=\"train_loss\")\n",
    "    plt.plot(history.get(\"val_loss\", []), label=\"val_loss\")\n",
    "    plt.xlabel(\"epoch\"); plt.ylabel(\"loss\"); plt.legend(); plt.grid(True)\n",
    "    f1 = out_dir / \"loss_curve.png\"\n",
    "    plt.savefig(f1); plt.close()\n",
    "    # metrics\n",
    "    for m in history.get(\"metrics\", {}).keys():\n",
    "        plt.figure()\n",
    "        plt.plot(history[\"metrics\"][m], label=m)\n",
    "        plt.xlabel(\"epoch\"); plt.ylabel(m); plt.legend(); plt.grid(True)\n",
    "        plt.savefig(out_dir / f\"metric_{m}.png\")\n",
    "        plt.close()\n",
    "    return out_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b7601df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 8) High-level train & evaluate pipeline\n",
    "# ---------------------------\n",
    "def train_and_evaluate(\n",
    "    processed_root: str,\n",
    "    dataset_name: str,\n",
    "    train_config_path: str,\n",
    "    metrics_config_path: str,\n",
    "    output_dir: str = \"model_output\",\n",
    "    mlflow_enabled: bool = False,\n",
    "    seed: int = 42\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    High-level entrypoint for training and evaluation. This is Airflow callable.\n",
    "    Returns a summary dict with final metrics and paths.\n",
    "    \"\"\"\n",
    "\n",
    "    # load configs\n",
    "    train_cfg, metrics_cfg = load_configs(train_config_path, metrics_config_path)\n",
    "    # set seeds\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # create train/test splits if needed\n",
    "    create_train_test_splits(processed_root, dataset_name, train_frac=float(train_cfg.get(\"train_frac\", 0.9)), seed=seed)\n",
    "\n",
    "    # dataset & dataloaders\n",
    "    train_dataset = CleanNoisyDataset(processed_root, dataset_name, split=\"train\")\n",
    "    val_dataset = CleanNoisyDataset(processed_root, dataset_name, split=\"test\")\n",
    "    train_loader = DataLoader(train_dataset, batch_size=train_cfg[\"batch_size\"], shuffle=True, num_workers=int(train_cfg.get(\"num_workers\", 4)), pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=train_cfg[\"batch_size\"], shuffle=False, num_workers=int(train_cfg.get(\"num_workers\", 4)), pin_memory=True)\n",
    "\n",
    "    # model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = WaveUNet1D(input_channels=1,\n",
    "                       output_channels=1,\n",
    "                       base_filters=int(train_cfg.get(\"base_filters\", 24)),\n",
    "                       depth=int(train_cfg.get(\"depth\", 5)),\n",
    "                       kernel_size=int(train_cfg.get(\"kernel_size\", 15)))\n",
    "    model.to(device)\n",
    "\n",
    "    # criterion, optimizer, scheduler\n",
    "    criterion = nn.L1Loss() if train_cfg.get(\"loss\", \"l1\") == \"l1\" else nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=float(train_cfg.get(\"lr\", 1e-4)))\n",
    "    scheduler = None\n",
    "    if train_cfg.get(\"lr_step\"):\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=train_cfg[\"lr_step\"], gamma=float(train_cfg.get(\"lr_gamma\", 0.5)))\n",
    "\n",
    "    # metrics and monitoring\n",
    "    metric_names = metrics_cfg.get(\"metrics\", [\"mse\"])\n",
    "    monitor = train_cfg.get(\"monitor_metric\", \"val_loss\")  # e.g. \"val_loss\" or \"si_sdr\"\n",
    "    monitor_mode = train_cfg.get(\"monitor_mode\", \"min\")   # \"min\" or \"max\"\n",
    "    best_score = math.inf if monitor_mode == \"min\" else -math.inf\n",
    "    best_ckpt_path = None\n",
    "\n",
    "    # output dirs\n",
    "    out_root = Path(output_dir)\n",
    "    ckpt_dir = out_root / \"checkpoints\"; ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "    plot_dir = out_root / \"plots\"; plot_dir.mkdir(parents=True, exist_ok=True)\n",
    "    history = {\"train_loss\": [], \"val_loss\": [], \"metrics\": {m: [] for m in metric_names}}\n",
    "\n",
    "    # MLflow start run\n",
    "    mlflow_run = None\n",
    "    if mlflow_enabled:\n",
    "        if not _mlflow_available:\n",
    "            logger.warning(\"MLflow requested but not available; continuing without MLflow.\")\n",
    "            mlflow_enabled = False\n",
    "        else:\n",
    "            mlflow.start_run()\n",
    "            mlflow_run = mlflow.active_run()\n",
    "            mlflow.log_params(train_cfg)\n",
    "            mlflow.log_params({\"metrics_cfg\": metrics_cfg})\n",
    "\n",
    "    num_epochs = int(train_cfg.get(\"epochs\", 50))\n",
    "    for epoch in range(num_epochs):\n",
    "        logger.info(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        t0 = time.time()\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, criterion, device, clip_grad=float(train_cfg.get(\"clip_grad\", 5.0)))\n",
    "        val_loss, val_metrics = validate_epoch(model, val_loader, criterion, device, metric_names)\n",
    "        logger.info(f\"Epoch {epoch+1} train_loss={train_loss:.6f} val_loss={val_loss:.6f} metrics={val_metrics} time={(time.time()-t0):.1f}s\")\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        for m in metric_names:\n",
    "            history[\"metrics\"][m].append(val_metrics.get(m, None))\n",
    "\n",
    "        # scheduler step\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        # monitoring and checkpointing\n",
    "        # If monitor_metric is \"val_loss\" use that; if it's in val_metrics use that.\n",
    "        if monitor == \"val_loss\":\n",
    "            current = val_loss\n",
    "        else:\n",
    "            current = val_metrics.get(monitor)\n",
    "            if current is None:\n",
    "                logger.warning(f\"Monitor metric {monitor} not found in val metrics; defaulting to val_loss.\")\n",
    "                current = val_loss\n",
    "\n",
    "        is_better = (current < best_score) if monitor_mode == \"min\" else (current > best_score)\n",
    "        if is_better:\n",
    "            best_score = current\n",
    "            best_ckpt_path = ckpt_dir / f\"best_{monitor}_{epoch+1}.pt\"\n",
    "            torch.save({\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"train_cfg\": train_cfg,\n",
    "                \"metrics_cfg\": metrics_cfg,\n",
    "                \"best_score\": best_score\n",
    "            }, str(best_ckpt_path))\n",
    "            logger.info(f\"Saved new best checkpoint: {best_ckpt_path}\")\n",
    "            if mlflow_enabled:\n",
    "                mlflow.log_metric(f\"best_{monitor}\", float(best_score), step=epoch)\n",
    "\n",
    "        # log epoch metrics to mlflow\n",
    "        if mlflow_enabled:\n",
    "            mlflow.log_metric(\"train_loss\", float(train_loss), step=epoch)\n",
    "            mlflow.log_metric(\"val_loss\", float(val_loss), step=epoch)\n",
    "            for m, v in val_metrics.items():\n",
    "                mlflow.log_metric(m, float(v), step=epoch)\n",
    "\n",
    "    # after training: load best model and evaluate on test set (here val set is test)\n",
    "    if best_ckpt_path is not None:\n",
    "        ckpt = torch.load(str(best_ckpt_path), map_location=device)\n",
    "        model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "        logger.info(f\"Loaded best model from {best_ckpt_path} for final evaluation.\")\n",
    "    else:\n",
    "        logger.warning(\"No checkpoint saved during training; using last model for evaluation.\")\n",
    "\n",
    "    # Final evaluation (on val/test loader)\n",
    "    final_loss, final_metrics = validate_epoch(model, val_loader, criterion, device, metric_names)\n",
    "    logger.info(f\"Final evaluation: loss={final_loss:.6f}, metrics={final_metrics}\")\n",
    "\n",
    "    # Save history & plots\n",
    "    hist_json = out_root / \"history.json\"\n",
    "    hist_json.write_text(json.dumps(history, indent=2))\n",
    "    plot_history(history, plot_dir)\n",
    "\n",
    "    # MLflow final logging & artifacts\n",
    "    if mlflow_enabled:\n",
    "        mlflow.log_metric(\"final_val_loss\", float(final_loss))\n",
    "        for m, v in final_metrics.items():\n",
    "            mlflow.log_metric(f\"final_{m}\", float(v))\n",
    "        # log artifacts\n",
    "        mlflow.log_artifacts(str(plot_dir), artifact_path=\"plots\")\n",
    "        mlflow.log_artifact(str(hist_json), artifact_path=\"history\")\n",
    "        if best_ckpt_path:\n",
    "            mlflow.log_artifact(str(best_ckpt_path), artifact_path=\"checkpoints\")\n",
    "        mlflow.end_run()\n",
    "\n",
    "    summary = {\n",
    "        \"best_checkpoint\": str(best_ckpt_path) if best_ckpt_path else None,\n",
    "        \"final_val_loss\": float(final_loss),\n",
    "        \"final_metrics\": final_metrics,\n",
    "        \"history_path\": str(hist_json),\n",
    "        \"plot_dir\": str(plot_dir)\n",
    "    }\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c346a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:model_pipeline:Train/test subfolders already exist; skipping split creation.\n",
      "INFO:model_pipeline:Epoch 1/40\n",
      "train:   0%|          | 0/113 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(\n",
    "    processed_root=\"guitar_dataset/processed\",\n",
    "    dataset_name=\"dataset2\",\n",
    "    train_config_path=\"train_config.yaml\",\n",
    "    metrics_config_path=\"metrics_config.yaml\",\n",
    "    mlflow_enabled=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cf4f07",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e978ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference_pipeline.py\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Optional, Dict\n",
    "\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e3fc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------\n",
    "# Helpers: chunking & IO\n",
    "# ------------------------\n",
    "def split_audio_into_chunks(\n",
    "    audio: np.ndarray,\n",
    "    sample_rate: int,\n",
    "    chunk_duration: float\n",
    ") -> Tuple[List[np.ndarray], int]:\n",
    "    \"\"\"\n",
    "    Split 1D numpy audio into non-overlapping chunks of chunk_duration (seconds).\n",
    "    Pads the last chunk with zeros if needed.\n",
    "\n",
    "    Returns:\n",
    "      chunks: list of numpy arrays (each length == chunk_len)\n",
    "      orig_len: original audio length in samples (for trimming after reconstruction)\n",
    "    \"\"\"\n",
    "    if audio.ndim > 1:\n",
    "        audio = np.mean(audio, axis=1)  # to mono\n",
    "\n",
    "    orig_len = len(audio)\n",
    "    chunk_len = int(round(chunk_duration * sample_rate))\n",
    "    if chunk_len <= 0:\n",
    "        raise ValueError(\"chunk_duration too small for given sample_rate\")\n",
    "\n",
    "    n_chunks = (orig_len + chunk_len - 1) // chunk_len\n",
    "    chunks = []\n",
    "    for i in range(n_chunks):\n",
    "        s = i * chunk_len\n",
    "        e = s + chunk_len\n",
    "        if e <= orig_len:\n",
    "            chunks.append(audio[s:e].astype(np.float32))\n",
    "        else:\n",
    "            # pad right\n",
    "            pad = e - orig_len\n",
    "            chunk = np.pad(audio[s:orig_len], (0, pad), mode='constant').astype(np.float32)\n",
    "            chunks.append(chunk)\n",
    "    return chunks, orig_len\n",
    "\n",
    "\n",
    "def reconstruct_from_chunks(chunks_preds: List[np.ndarray], orig_len: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Concatenate list of 1D arrays (all same length) and trim to orig_len samples.\n",
    "    \"\"\"\n",
    "    if not chunks_preds:\n",
    "        return np.zeros(orig_len, dtype=np.float32)\n",
    "    out = np.concatenate(chunks_preds, axis=0)\n",
    "    if len(out) >= orig_len:\n",
    "        return out[:orig_len].astype(np.float32)\n",
    "    # if concatenated length is shorter (shouldn't happen), pad\n",
    "    pad = orig_len - len(out)\n",
    "    return np.pad(out, (0, pad), mode='constant').astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a51c591",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Optional' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ------------------------\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Model loading\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# ------------------------\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_best_model\u001b[39m(checkpoint_path: \u001b[38;5;28mstr\u001b[39m, device: \u001b[43mOptional\u001b[49m[torch.device] = \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m      5\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[33;03m    Loads a checkpoint saved by train_and_evaluate(...) and returns a model in eval mode.\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[33;03m    The checkpoint is expected to contain 'model_state_dict' and optionally 'train_cfg' dict.\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[33;03m    If train_cfg exists, it is used to re-create the WaveUNet1D with the same architecture.\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m     10\u001b[39m     device = device \u001b[38;5;129;01mor\u001b[39;00m (torch.device(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m torch.device(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[31mNameError\u001b[39m: name 'Optional' is not defined"
     ]
    }
   ],
   "source": [
    "# ------------------------\n",
    "# Model loading\n",
    "# ------------------------\n",
    "def load_best_model(checkpoint_path: str, device: Optional[torch.device] = None):\n",
    "    \"\"\"\n",
    "    Loads a checkpoint saved by train_and_evaluate(...) and returns a model in eval mode.\n",
    "    The checkpoint is expected to contain 'model_state_dict' and optionally 'train_cfg' dict.\n",
    "    If train_cfg exists, it is used to re-create the WaveUNet1D with the same architecture.\n",
    "    \"\"\"\n",
    "    device = device or (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
    "    ckpt = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "    # try to infer model config\n",
    "    train_cfg = ckpt.get(\"train_cfg\") or ckpt.get(\"args\") or {}\n",
    "    base_filters = int(train_cfg.get(\"base_filters\", 24))\n",
    "    depth = int(train_cfg.get(\"depth\", 5))\n",
    "    kernel_size = int(train_cfg.get(\"kernel_size\", 15))\n",
    "    input_channels = int(train_cfg.get(\"input_channels\", 1))\n",
    "    output_channels = int(train_cfg.get(\"output_channels\", 1))\n",
    "\n",
    "    model = WaveUNet1D(input_channels=input_channels,\n",
    "                       output_channels=output_channels,\n",
    "                       base_filters=base_filters,\n",
    "                       depth=depth,\n",
    "                       kernel_size=kernel_size)\n",
    "    model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model, train_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dd8749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------\n",
    "# Core inference for a single recording\n",
    "# ------------------------\n",
    "def infer_single_recording(\n",
    "    input_path: str,\n",
    "    model: torch.nn.Module,\n",
    "    sample_rate: int,\n",
    "    chunk_duration: float,\n",
    "    batch_size: int,\n",
    "    device: torch.device,\n",
    "    output_dir: str,\n",
    "    checkpoint_path: str,\n",
    "    dtype=np.float32\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Process one recording: split into chunks, run model in batches, reconstruct, save output and metadata.\n",
    "\n",
    "    Returns metadata dict with paths and basic stats.\n",
    "    \"\"\"\n",
    "    input_path = Path(input_path)\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # load and ensure mono\n",
    "    audio, sr = librosa.load(str(input_path), sr=sample_rate, mono=True)\n",
    "    chunks, orig_len = split_audio_into_chunks(audio, sample_rate, chunk_duration)\n",
    "\n",
    "    preds = []\n",
    "    model_device = device\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(chunks), batch_size):\n",
    "            batch_chunks = chunks[i:i + batch_size]\n",
    "            # build tensor shape (B,1,L)\n",
    "            batch_arr = np.stack(batch_chunks, axis=0)  # (B, L)\n",
    "            batch_tensor = torch.from_numpy(batch_arr).float().unsqueeze(1).to(model_device)  # (B,1,L)\n",
    "\n",
    "            out_tensor = model(batch_tensor)  # expected (B,1,L) thanks to model alignment\n",
    "            out_tensor = out_tensor.detach().cpu().numpy()  # (B,1,L)\n",
    "            out_arrs = [o[0].astype(dtype) for o in out_tensor]  # list of 1D arrays\n",
    "            preds.extend(out_arrs)\n",
    "\n",
    "    # reconstruct\n",
    "    reconstructed = reconstruct_from_chunks(preds, orig_len)\n",
    "\n",
    "    # save\n",
    "    out_basename = input_path.stem + \"_recon.wav\"\n",
    "    out_path = output_dir / out_basename\n",
    "    sf.write(str(out_path), reconstructed, sample_rate)\n",
    "\n",
    "    # save metadata\n",
    "    meta = {\n",
    "        \"input_path\": str(input_path),\n",
    "        \"output_path\": str(out_path),\n",
    "        \"checkpoint\": str(checkpoint_path),\n",
    "        \"sample_rate\": int(sample_rate),\n",
    "        \"chunk_duration\": float(chunk_duration),\n",
    "        \"n_chunks\": int(len(chunks)),\n",
    "        \"orig_len_samples\": int(orig_len),\n",
    "        \"created_at\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime())\n",
    "    }\n",
    "    meta_path = output_dir / (input_path.stem + \"_meta.json\")\n",
    "    with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(meta, f, indent=2)\n",
    "\n",
    "    return meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2691f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------\n",
    "# Top-level Airflow-friendly entrypoint\n",
    "# ------------------------\n",
    "def run_inference_pipeline(\n",
    "    input_root: str,\n",
    "    model_checkpoint: str,\n",
    "    output_root: str,\n",
    "    chunk_duration: float = 3.0,\n",
    "    sample_rate: int = 22050,\n",
    "    batch_size: int = 8,\n",
    "    device_str: Optional[str] = None,\n",
    "    glob_patterns: Optional[List[str]] = None,\n",
    "    max_files: Optional[int] = None\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Main function to be called by Airflow PythonOperator.\n",
    "\n",
    "    Parameters:\n",
    "      - input_root: folder with recordings (will search recursively)\n",
    "      - model_checkpoint: path to checkpoint (best model)\n",
    "      - output_root: where to save reconstructed recordings and metadata\n",
    "      - chunk_duration: seconds per chunk to pass to model\n",
    "      - sample_rate: sampling rate for loading/saving\n",
    "      - batch_size: inference batch size\n",
    "      - device_str: 'cuda' or 'cpu' (if None, automatically picked)\n",
    "      - glob_patterns: list of glob patterns to find files (default ['**/*.wav'])\n",
    "      - max_files: optional cap on number of recordings to process\n",
    "\n",
    "    Returns:\n",
    "      summary dict with list of processed files and metadata paths.\n",
    "    \"\"\"\n",
    "    device = torch.device(device_str if device_str is not None else (\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    model, train_cfg = load_best_model(model_checkpoint, device=device)\n",
    "\n",
    "    input_root = Path(input_root)\n",
    "    out_root = Path(output_root)\n",
    "    out_root.mkdir(parents=True, exist_ok=True)\n",
    "    glob_patterns = glob_patterns or [\"**/*.wav\"]\n",
    "\n",
    "    # collect files\n",
    "    files = []\n",
    "    for pat in glob_patterns:\n",
    "        files.extend(sorted(input_root.glob(pat)))\n",
    "    if not files:\n",
    "        raise RuntimeError(f\"No audio files found in {input_root} with patterns {glob_patterns}\")\n",
    "\n",
    "    if max_files is not None:\n",
    "        files = files[:max_files]\n",
    "\n",
    "    processed = []\n",
    "    pbar = tqdm(files, desc=\"Inference files\", dynamic_ncols=True)\n",
    "    for f in pbar:\n",
    "        try:\n",
    "            meta = infer_single_recording(\n",
    "                input_path=str(f),\n",
    "                model=model,\n",
    "                sample_rate=sample_rate,\n",
    "                chunk_duration=chunk_duration,\n",
    "                batch_size=batch_size,\n",
    "                device=device,\n",
    "                output_dir=str(out_root),\n",
    "                checkpoint_path=model_checkpoint\n",
    "            )\n",
    "            processed.append(meta)\n",
    "        except Exception as e:\n",
    "            # don't crash whole DAG on single file; instead record error\n",
    "            processed.append({\"input_path\": str(f), \"error\": str(e)})\n",
    "            # you may also choose to re-raise if you want failure semantics in Airflow\n",
    "            # raise\n",
    "\n",
    "    summary = {\n",
    "        \"model_checkpoint\": str(model_checkpoint),\n",
    "        \"n_files_requested\": len(files),\n",
    "        \"n_processed\": len(processed),\n",
    "        \"output_root\": str(out_root),\n",
    "        \"processed\": processed\n",
    "    }\n",
    "    # save summary\n",
    "    with open(out_root / \"inference_summary.json\", \"w\", encoding=\"utf-8\") as fh:\n",
    "        json.dump(summary, fh, indent=2)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4045c6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------\n",
    "# Example Airflow PythonOperator usage:\n",
    "# ------------------------\n",
    "# from airflow import DAG\n",
    "# from airflow.operators.python import PythonOperator\n",
    "# from datetime import datetime\n",
    "#\n",
    "# with DAG(\"waveunet_inference\", start_date=datetime(2025,10,4), schedule=None, catchup=False) as dag:\n",
    "#     infer_task = PythonOperator(\n",
    "#         task_id=\"run_inference\",\n",
    "#         python_callable=run_inference_pipeline,\n",
    "#         op_kwargs={\n",
    "#             \"input_root\": \"/mnt/data/to_process\",\n",
    "#             \"model_checkpoint\": \"/path/to/best_checkpoint.pt\",\n",
    "#             \"output_root\": \"/mnt/data/inference_out\",\n",
    "#             \"chunk_duration\": 3.0,\n",
    "#             \"sample_rate\": 22050,\n",
    "#             \"batch_size\": 8,\n",
    "#             \"device_str\": \"cuda\",\n",
    "#             \"glob_patterns\": [\"**/*.wav\"],\n",
    "#             \"max_files\": 200\n",
    "#         },\n",
    "#     )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
